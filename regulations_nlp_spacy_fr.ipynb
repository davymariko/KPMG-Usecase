{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "regulations_nlp_spacy_fr.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPy5Micuk78q8f323EMqgfM"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kuKWsJV27Xe2"
      },
      "source": [
        "# Importing and preprocessing of data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OHoCSdHswimk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05a41962-3749-4327-a35b-66b1dfb995f4"
      },
      "source": [
        "from google.colab import drive\r\n",
        "\r\n",
        "drive.mount('/content/drive/')\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMVNjvlExvOf"
      },
      "source": [
        "TXTS_PATH = '/content/drive/My Drive/Becode/Kpmg/selection_fr'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AxwxF21YxnUj"
      },
      "source": [
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JZx91aOoyi1d"
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J_t3pBX-xF7U"
      },
      "source": [
        "RESPONSES_PATH = '/content/drive/My Drive/Becode/Kpmg/responses_2018_now.json'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DkiARh2Oxg4o"
      },
      "source": [
        "txt_filenames = [f for f in os.listdir(TXTS_PATH) if f.endswith(\".txt\")]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZddQl2FVydlE"
      },
      "source": [
        "#reading json file to retrieve tags\r\n",
        "responses = pd.read_json(RESPONSES_PATH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "ycPxUepwyxtL",
        "outputId": "bdc07cbb-310f-43f9-cf43-a6740d39c96f"
      },
      "source": [
        "#filtering only relevant columns\r\n",
        "columns = ['jcId', 'jcFr', 'titleFr', 'themesFr', 'scopeFr', 'noScopeFr', 'documentLink']\r\n",
        "responses2 = responses.copy(deep=True).loc[:,columns]\r\n",
        "#getting txt_name from original pdf name\r\n",
        "responses2[\"txt_name\"] = responses2[\"documentLink\"]\r\n",
        "responses2[\"txt_name\"] = responses2[\"txt_name\"].str.replace(\"/\",\"-\")\r\n",
        "responses2[\"txt_name\"] = responses2[\"txt_name\"].str.replace(\".pdf\",\"_FR.txt\")\r\n",
        "#filtering json only for selected files and columns\r\n",
        "columns += ['txt_name']\r\n",
        "responses2 = responses2.loc[responses2.txt_name.isin(txt_filenames), columns]\r\n",
        "responses2.head(2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>jcId</th>\n",
              "      <th>jcFr</th>\n",
              "      <th>titleFr</th>\n",
              "      <th>themesFr</th>\n",
              "      <th>scopeFr</th>\n",
              "      <th>noScopeFr</th>\n",
              "      <th>documentLink</th>\n",
              "      <th>txt_name</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>1110000</td>\n",
              "      <td>COMMISSION PARITAIRE DES CONSTRUCTIONS METALLI...</td>\n",
              "      <td>modification du régime de pension sectoriel so...</td>\n",
              "      <td>[PENSIONS COMPÉMENTAIRES ET ASSURANCES GROUPES]</td>\n",
              "      <td>None</td>\n",
              "      <td>[les employeurs et ouvriers des entreprises ex...</td>\n",
              "      <td>111/111-2018-013525.pdf</td>\n",
              "      <td>111-111-2018-013525_FR.txt</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>1110000</td>\n",
              "      <td>COMMISSION PARITAIRE DES CONSTRUCTIONS METALLI...</td>\n",
              "      <td>allocation spéciale compensatoire</td>\n",
              "      <td>[PRIME SYNDICALE]</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>111/111-2018-012196.pdf</td>\n",
              "      <td>111-111-2018-012196_FR.txt</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       jcId  ...                    txt_name\n",
              "34  1110000  ...  111-111-2018-013525_FR.txt\n",
              "35  1110000  ...  111-111-2018-012196_FR.txt\n",
              "\n",
              "[2 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XlXwURIgy2uP",
        "outputId": "cd82f3df-0e72-42d4-e14a-41ea03febf8c"
      },
      "source": [
        "#generating all possible FR themes\r\n",
        "themes_fr = []\r\n",
        "for r in  responses2.themesFr:\r\n",
        "    if r is not None:\r\n",
        "        for t in r:\r\n",
        "            if t not in themes_fr:\r\n",
        "                themes_fr += [t]\r\n",
        "len(themes_fr)\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "53"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r2kAiO8Uy9Eu"
      },
      "source": [
        "#code to retrieve the entire body\r\n",
        "#%%\r\n",
        "for filename in responses2.txt_name.to_list():\r\n",
        "    # filename = responses2.txt_name.to_list()[10] #test\r\n",
        "    file_path = os.path.join(TXTS_PATH, filename)\r\n",
        "    with open(file_path, 'r', encoding=\"utf8\") as f:\r\n",
        "        #to retrieve entire body not necessary\r\n",
        "        responses2.loc[responses2.txt_name == filename, \"doc_bodies\"] = f.read()\r\n",
        "        f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iox0MB-33HOD"
      },
      "source": [
        "t = themes_fr[0]\r\n",
        "mask = responses2.dropna(axis=0, subset=['themesFr']).themesFr.map(lambda x: t in x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NUAfSmIf16p-",
        "outputId": "b42629a3-063f-42de-b8af-f274c250c1ea"
      },
      "source": [
        "len(responses2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "549"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0kHR6RAx2OK0"
      },
      "source": [
        " #len(responses2.dropna(axis=0, subset=['themesFr'])[map])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H2PzphfD67lX"
      },
      "source": [
        "# Preprocessing for NLP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ib8xc7-F5tGY"
      },
      "source": [
        "from pandas import Series\r\n",
        "\r\n",
        "from nltk.corpus import wordnet\r\n",
        "from nltk.tokenize import word_tokenize\r\n",
        "from nltk.tag import pos_tag\r\n",
        "from nltk.stem import WordNetLemmatizer\r\n",
        "\r\n",
        "# import entirely spacy to create Doc objects through nlp\r\n",
        "import spacy\r\n",
        "from spacy import load, lang\r\n",
        "\r\n",
        "from wordcloud import WordCloud\r\n",
        "\r\n",
        "from collections import Counter\r\n",
        "\r\n",
        "from typing import List\r\n",
        "from typing import Dict\r\n",
        "\r\n",
        "# WARNINGS\r\n",
        "# W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found\r\n",
        "# I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine\r\n",
        "\r\n",
        "# GLOBAL VARIABLES\r\n",
        "NAMED_ENTITIES = ['microsoft']\r\n",
        "\r\n",
        "#testing\r\n",
        "from os import getcwd as cwd\r\n",
        "from os.path import dirname as dir\r\n",
        "from os.path import join\r\n",
        "import pandas as pd\r\n",
        "\r\n",
        "\r\n",
        "def lemmatize(text_tokens: List[str]) -> List[str]:\r\n",
        "    def get_wordnet_pos(word):\r\n",
        "        \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\r\n",
        "        tag = pos_tag([word])[0][1][0].upper()\r\n",
        "        tag_dict = {\"J\": wordnet.ADJ,\r\n",
        "                    \"N\": wordnet.NOUN,\r\n",
        "                    \"V\": wordnet.VERB,\r\n",
        "                    \"R\": wordnet.ADV}\r\n",
        "        return tag_dict.get(tag, wordnet.NOUN)\r\n",
        "\r\n",
        "    # Instantiate the WordNetLemmatizer\r\n",
        "    wordnet_lemmatizer = WordNetLemmatizer()\r\n",
        "    # Lemmatize all tokens into a new list: lemmatized\r\n",
        "    texts_lemmatized = [wordnet_lemmatizer.lemmatize(t, get_wordnet_pos(t)) for t in text_tokens]\r\n",
        "    return texts_lemmatized\r\n",
        "\r\n",
        "\r\n",
        "#DEV NOTE: not used\r\n",
        "def filter_words(texts_lemmatized: List[List[str]], freq_min=None, freq_max=None) -> List[List[str]]:\r\n",
        "    frequency_absolute = Counter([item for elem in texts_lemmatized for item in elem])\r\n",
        "    wordcloud = WordCloud(width=1000, height=500).generate_from_frequencies(frequency_absolute)\r\n",
        "    frequency_relative = wordcloud.words_\r\n",
        "    if freq_min is not None and freq_min > 0 and freq_min < 1:\r\n",
        "        rel_freq_filtered = {k: v for k, v in frequency_relative.items() if v > freq_min}\r\n",
        "    if freq_max is not None and freq_max > 0 and freq_max < 1:\r\n",
        "        rel_freq_filtered = {k: v for k, v in frequency_relative.items() if v < freq_max}\r\n",
        "    texts_filtered = [[t for t in pub_lem if t in rel_freq_filtered.keys()] for pub_lem in texts_lemmatized]\r\n",
        "    return texts_filtered\r\n",
        "\r\n",
        "\r\n",
        "class Preprocess:\r\n",
        "    def __init__(self, nlp_model='en_core_web_md'):\r\n",
        "        self.nlp = load(nlp_model)\r\n",
        "        if 'en_' in nlp_model:\r\n",
        "          self.stop_words = lang.en.stop_words.STOP_WORDS\r\n",
        "        elif 'fr_' in nlp_model:\r\n",
        "          self.stop_words = lang.fr.stop_words.STOP_WORDS\r\n",
        "        self.named_entities = set(NAMED_ENTITIES)\r\n",
        "\r\n",
        "    def get_named_entities(self, texts: Series, inplace=True) -> set:\r\n",
        "        # creating a single ner set\r\n",
        "        nes = set()\r\n",
        "        # function to extract NER from text\r\n",
        "        def get_named_entities(text) -> set:\r\n",
        "            doc = self.nlp(text)\r\n",
        "            named_entities = set([ent.text for ent in doc.ents])\r\n",
        "            return named_entities\r\n",
        "        [[nes.add(n) for n in get_named_entities(text)] for text in texts]\r\n",
        "        # adding predefined NER\r\n",
        "        [nes.add(n) for n in self.named_entities]\r\n",
        "        if inplace:\r\n",
        "            self.named_entities = nes\r\n",
        "        return nes\r\n",
        "\r\n",
        "    def tokenize_text(self, text:str, stop_words: List[str] = None, named_entities: List[str] = None,\r\n",
        "                   lenght_min: int=2) -> List[str]:\r\n",
        "        if stop_words is None:\r\n",
        "            stop_words = self.stop_words\r\n",
        "        if named_entities is None:\r\n",
        "            named_entities = self.named_entities\r\n",
        "        text = text.replace(\"\\n\", \" \")\r\n",
        "        # split string into words (tokens)\r\n",
        "        tokens = word_tokenize(text.lower())\r\n",
        "        # keep strings with only alphabets\r\n",
        "        tokens = [t for t in tokens if t.isalpha()]\r\n",
        "        tokens = lemmatize(tokens)\r\n",
        "        # remove short words, they're probably not useful\r\n",
        "        tokens = [t for t in tokens if len(t) > lenght_min]\r\n",
        "        # remove stopwords\r\n",
        "        tokens = [t for t in tokens if t not in stop_words]\r\n",
        "        # remove\r\n",
        "        tokens = [t for t in tokens if t not in named_entities]\r\n",
        "        return tokens\r\n",
        "\r\n",
        "    def clean_text(self, text:str, stop_words: List[str] = None, named_entities: List[str] = None,\r\n",
        "                   lenght_min: int=2) -> str:\r\n",
        "        tokens = self.tokenize_text(text, stop_words, named_entities, lenght_min)\r\n",
        "        text_cleaned = \" \".join(tokens)\r\n",
        "        return text_cleaned\r\n",
        "\r\n",
        "    def tokenize_texts(self, texts:Series, stop_words: List[str] = None, named_entities: List[str] = None,\r\n",
        "                   lenght_min: int=2) -> List[List[str]]:\r\n",
        "        texts_tokens = []\r\n",
        "        for text in texts:\r\n",
        "            texts_tokens = texts_tokens.append(self.tokenize_text(text, stop_words, named_entities, lenght_min))\r\n",
        "        return texts_tokens"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UzM0CO3T7Jp5",
        "outputId": "20b3b647-4af0-437c-ab0e-ef06233e6537"
      },
      "source": [
        "! python -m spacy download fr_core_news_md"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting fr_core_news_md==2.2.5\n",
            "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/fr_core_news_md-2.2.5/fr_core_news_md-2.2.5.tar.gz (88.6MB)\n",
            "\u001b[K     |████████████████████████████████| 88.6MB 1.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from fr_core_news_md==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->fr_core_news_md==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->fr_core_news_md==2.2.5) (54.0.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->fr_core_news_md==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->fr_core_news_md==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->fr_core_news_md==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->fr_core_news_md==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->fr_core_news_md==2.2.5) (1.19.5)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->fr_core_news_md==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->fr_core_news_md==2.2.5) (2.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->fr_core_news_md==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->fr_core_news_md==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->fr_core_news_md==2.2.5) (3.0.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->fr_core_news_md==2.2.5) (0.8.2)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->fr_core_news_md==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->fr_core_news_md==2.2.5) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->fr_core_news_md==2.2.5) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->fr_core_news_md==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->fr_core_news_md==2.2.5) (3.7.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->fr_core_news_md==2.2.5) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->fr_core_news_md==2.2.5) (3.4.0)\n",
            "Building wheels for collected packages: fr-core-news-md\n",
            "  Building wheel for fr-core-news-md (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fr-core-news-md: filename=fr_core_news_md-2.2.5-cp37-none-any.whl size=90338490 sha256=b795eafecf77beb60abacab5ab2980db9e586ea4bfcfa8646c04fbd57187e33e\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-ir98s7j9/wheels/c6/18/b6/f628642acc7872a53cf81269dd1c394d96da69564ccfac5425\n",
            "Successfully built fr-core-news-md\n",
            "Installing collected packages: fr-core-news-md\n",
            "Successfully installed fr-core-news-md-2.2.5\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('fr_core_news_md')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-gSiOJ547ivj"
      },
      "source": [
        "import spacy\r\n",
        "nlp = spacy.load('fr_core_news_md')"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1qD-aTcn8atq"
      },
      "source": [
        "! python --version"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        },
        "id": "m7pS6KZYHixh",
        "outputId": "f89b8c26-3a30-4187-9c09-1e9bf9080c95"
      },
      "source": [
        "preprocess = Preprocess(nlp_model='fr_core_news_md')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-1113b20303f3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpreprocess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnlp_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'fr_core_news_md'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-3-0156e6e507c7>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, nlp_model)\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnlp_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'en_core_web_md'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnlp_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0men\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_words\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSTOP_WORDS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamed_entities\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNAMED_ENTITIES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'spacy.lang' has no attribute 'en'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dZ1u8dKqIOi2",
        "outputId": "37bb0e2c-1d48-454a-9c97-64b0adcf07ae"
      },
      "source": [
        "lang.fr.stop_words.STOP_WORDS"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'a',\n",
              " 'abord',\n",
              " 'absolument',\n",
              " 'afin',\n",
              " 'ah',\n",
              " 'ai',\n",
              " 'aie',\n",
              " 'ailleurs',\n",
              " 'ainsi',\n",
              " 'ait',\n",
              " 'allaient',\n",
              " 'allo',\n",
              " 'allons',\n",
              " 'allô',\n",
              " 'alors',\n",
              " 'anterieur',\n",
              " 'anterieure',\n",
              " 'anterieures',\n",
              " 'apres',\n",
              " 'après',\n",
              " 'as',\n",
              " 'assez',\n",
              " 'attendu',\n",
              " 'au',\n",
              " 'aucun',\n",
              " 'aucune',\n",
              " 'aujourd',\n",
              " \"aujourd'hui\",\n",
              " 'aupres',\n",
              " 'auquel',\n",
              " 'aura',\n",
              " 'auraient',\n",
              " 'aurait',\n",
              " 'auront',\n",
              " 'aussi',\n",
              " 'autre',\n",
              " 'autrefois',\n",
              " 'autrement',\n",
              " 'autres',\n",
              " 'autrui',\n",
              " 'aux',\n",
              " 'auxquelles',\n",
              " 'auxquels',\n",
              " 'avaient',\n",
              " 'avais',\n",
              " 'avait',\n",
              " 'avant',\n",
              " 'avec',\n",
              " 'avoir',\n",
              " 'avons',\n",
              " 'ayant',\n",
              " 'bah',\n",
              " 'bas',\n",
              " 'basee',\n",
              " 'bat',\n",
              " 'beau',\n",
              " 'beaucoup',\n",
              " 'bien',\n",
              " 'bigre',\n",
              " 'boum',\n",
              " 'bravo',\n",
              " 'brrr',\n",
              " \"c'\",\n",
              " 'car',\n",
              " 'ce',\n",
              " 'ceci',\n",
              " 'cela',\n",
              " 'celle',\n",
              " 'celle-ci',\n",
              " 'celle-là',\n",
              " 'celles',\n",
              " 'celles-ci',\n",
              " 'celles-là',\n",
              " 'celui',\n",
              " 'celui-ci',\n",
              " 'celui-là',\n",
              " 'cent',\n",
              " 'cependant',\n",
              " 'certain',\n",
              " 'certaine',\n",
              " 'certaines',\n",
              " 'certains',\n",
              " 'certes',\n",
              " 'ces',\n",
              " 'cet',\n",
              " 'cette',\n",
              " 'ceux',\n",
              " 'ceux-ci',\n",
              " 'ceux-là',\n",
              " 'chacun',\n",
              " 'chacune',\n",
              " 'chaque',\n",
              " 'cher',\n",
              " 'chers',\n",
              " 'chez',\n",
              " 'chiche',\n",
              " 'chut',\n",
              " 'chère',\n",
              " 'chères',\n",
              " 'ci',\n",
              " 'cinq',\n",
              " 'cinquantaine',\n",
              " 'cinquante',\n",
              " 'cinquantième',\n",
              " 'cinquième',\n",
              " 'clac',\n",
              " 'clic',\n",
              " 'combien',\n",
              " 'comme',\n",
              " 'comment',\n",
              " 'comparable',\n",
              " 'comparables',\n",
              " 'compris',\n",
              " 'concernant',\n",
              " 'contre',\n",
              " 'couic',\n",
              " 'crac',\n",
              " 'c’',\n",
              " \"d'\",\n",
              " 'da',\n",
              " 'dans',\n",
              " 'de',\n",
              " 'debout',\n",
              " 'dedans',\n",
              " 'dehors',\n",
              " 'deja',\n",
              " 'delà',\n",
              " 'depuis',\n",
              " 'dernier',\n",
              " 'derniere',\n",
              " 'derriere',\n",
              " 'derrière',\n",
              " 'des',\n",
              " 'desormais',\n",
              " 'desquelles',\n",
              " 'desquels',\n",
              " 'dessous',\n",
              " 'dessus',\n",
              " 'deux',\n",
              " 'deuxième',\n",
              " 'deuxièmement',\n",
              " 'devant',\n",
              " 'devers',\n",
              " 'devra',\n",
              " 'different',\n",
              " 'differentes',\n",
              " 'differents',\n",
              " 'différent',\n",
              " 'différente',\n",
              " 'différentes',\n",
              " 'différents',\n",
              " 'dire',\n",
              " 'directe',\n",
              " 'directement',\n",
              " 'dit',\n",
              " 'dite',\n",
              " 'dits',\n",
              " 'divers',\n",
              " 'diverse',\n",
              " 'diverses',\n",
              " 'dix',\n",
              " 'dix-huit',\n",
              " 'dix-neuf',\n",
              " 'dix-sept',\n",
              " 'dixième',\n",
              " 'doit',\n",
              " 'doivent',\n",
              " 'donc',\n",
              " 'dont',\n",
              " 'douze',\n",
              " 'douzième',\n",
              " 'dring',\n",
              " 'du',\n",
              " 'duquel',\n",
              " 'durant',\n",
              " 'dès',\n",
              " 'désormais',\n",
              " 'd’',\n",
              " 'effet',\n",
              " 'egale',\n",
              " 'egalement',\n",
              " 'egales',\n",
              " 'eh',\n",
              " 'elle',\n",
              " 'elle-même',\n",
              " 'elles',\n",
              " 'elles-mêmes',\n",
              " 'en',\n",
              " 'encore',\n",
              " 'enfin',\n",
              " 'entre',\n",
              " 'envers',\n",
              " 'environ',\n",
              " 'es',\n",
              " 'est',\n",
              " 'et',\n",
              " 'etaient',\n",
              " 'etais',\n",
              " 'etait',\n",
              " 'etant',\n",
              " 'etc',\n",
              " 'etre',\n",
              " 'eu',\n",
              " 'euh',\n",
              " 'eux',\n",
              " 'eux-mêmes',\n",
              " 'exactement',\n",
              " 'excepté',\n",
              " 'extenso',\n",
              " 'exterieur',\n",
              " 'fais',\n",
              " 'faisaient',\n",
              " 'faisant',\n",
              " 'fait',\n",
              " 'façon',\n",
              " 'feront',\n",
              " 'fi',\n",
              " 'flac',\n",
              " 'floc',\n",
              " 'font',\n",
              " 'gens',\n",
              " 'ha',\n",
              " 'hein',\n",
              " 'hem',\n",
              " 'hep',\n",
              " 'hi',\n",
              " 'ho',\n",
              " 'holà',\n",
              " 'hop',\n",
              " 'hormis',\n",
              " 'hors',\n",
              " 'hou',\n",
              " 'houp',\n",
              " 'hue',\n",
              " 'hui',\n",
              " 'huit',\n",
              " 'huitième',\n",
              " 'hum',\n",
              " 'hurrah',\n",
              " 'hé',\n",
              " 'hélas',\n",
              " 'i',\n",
              " 'il',\n",
              " 'ils',\n",
              " 'importe',\n",
              " \"j'\",\n",
              " 'je',\n",
              " 'jusqu',\n",
              " 'jusque',\n",
              " 'juste',\n",
              " 'j’',\n",
              " \"l'\",\n",
              " 'la',\n",
              " 'laisser',\n",
              " 'laquelle',\n",
              " 'las',\n",
              " 'le',\n",
              " 'lequel',\n",
              " 'les',\n",
              " 'lesquelles',\n",
              " 'lesquels',\n",
              " 'leur',\n",
              " 'leurs',\n",
              " 'longtemps',\n",
              " 'lors',\n",
              " 'lorsque',\n",
              " 'lui',\n",
              " 'lui-meme',\n",
              " 'lui-même',\n",
              " 'là',\n",
              " 'lès',\n",
              " 'l’',\n",
              " \"m'\",\n",
              " 'ma',\n",
              " 'maint',\n",
              " 'maintenant',\n",
              " 'mais',\n",
              " 'malgre',\n",
              " 'malgré',\n",
              " 'maximale',\n",
              " 'me',\n",
              " 'meme',\n",
              " 'memes',\n",
              " 'merci',\n",
              " 'mes',\n",
              " 'mien',\n",
              " 'mienne',\n",
              " 'miennes',\n",
              " 'miens',\n",
              " 'mille',\n",
              " 'mince',\n",
              " 'minimale',\n",
              " 'moi',\n",
              " 'moi-meme',\n",
              " 'moi-même',\n",
              " 'moindres',\n",
              " 'moins',\n",
              " 'mon',\n",
              " 'moyennant',\n",
              " 'même',\n",
              " 'mêmes',\n",
              " 'm’',\n",
              " \"n'\",\n",
              " 'na',\n",
              " 'naturel',\n",
              " 'naturelle',\n",
              " 'naturelles',\n",
              " 'ne',\n",
              " 'neanmoins',\n",
              " 'necessaire',\n",
              " 'necessairement',\n",
              " 'neuf',\n",
              " 'neuvième',\n",
              " 'ni',\n",
              " 'nombreuses',\n",
              " 'nombreux',\n",
              " 'non',\n",
              " 'nos',\n",
              " 'notamment',\n",
              " 'notre',\n",
              " 'nous',\n",
              " 'nous-mêmes',\n",
              " 'nouveau',\n",
              " 'nul',\n",
              " 'néanmoins',\n",
              " 'nôtre',\n",
              " 'nôtres',\n",
              " 'n’',\n",
              " 'o',\n",
              " 'oh',\n",
              " 'ohé',\n",
              " 'ollé',\n",
              " 'olé',\n",
              " 'on',\n",
              " 'ont',\n",
              " 'onze',\n",
              " 'onzième',\n",
              " 'ore',\n",
              " 'ou',\n",
              " 'ouf',\n",
              " 'ouias',\n",
              " 'oust',\n",
              " 'ouste',\n",
              " 'outre',\n",
              " 'ouvert',\n",
              " 'ouverte',\n",
              " 'ouverts',\n",
              " 'où',\n",
              " 'paf',\n",
              " 'pan',\n",
              " 'par',\n",
              " 'parce',\n",
              " 'parfois',\n",
              " 'parle',\n",
              " 'parlent',\n",
              " 'parler',\n",
              " 'parmi',\n",
              " 'parseme',\n",
              " 'partant',\n",
              " 'particulier',\n",
              " 'particulière',\n",
              " 'particulièrement',\n",
              " 'pas',\n",
              " 'passé',\n",
              " 'pendant',\n",
              " 'pense',\n",
              " 'permet',\n",
              " 'personne',\n",
              " 'peu',\n",
              " 'peut',\n",
              " 'peuvent',\n",
              " 'peux',\n",
              " 'pff',\n",
              " 'pfft',\n",
              " 'pfut',\n",
              " 'pif',\n",
              " 'pire',\n",
              " 'plein',\n",
              " 'plouf',\n",
              " 'plus',\n",
              " 'plusieurs',\n",
              " 'plutôt',\n",
              " 'possessif',\n",
              " 'possessifs',\n",
              " 'possible',\n",
              " 'possibles',\n",
              " 'pouah',\n",
              " 'pour',\n",
              " 'pourquoi',\n",
              " 'pourrais',\n",
              " 'pourrait',\n",
              " 'pouvait',\n",
              " 'prealable',\n",
              " 'precisement',\n",
              " 'premier',\n",
              " 'première',\n",
              " 'premièrement',\n",
              " 'pres',\n",
              " 'probable',\n",
              " 'probante',\n",
              " 'procedant',\n",
              " 'proche',\n",
              " 'près',\n",
              " 'psitt',\n",
              " 'pu',\n",
              " 'puis',\n",
              " 'puisque',\n",
              " 'pur',\n",
              " 'pure',\n",
              " \"qu'\",\n",
              " 'quand',\n",
              " 'quant',\n",
              " 'quant-à-soi',\n",
              " 'quanta',\n",
              " 'quarante',\n",
              " 'quatorze',\n",
              " 'quatre',\n",
              " 'quatre-vingt',\n",
              " 'quatrième',\n",
              " 'quatrièmement',\n",
              " 'que',\n",
              " 'quel',\n",
              " 'quelconque',\n",
              " 'quelle',\n",
              " 'quelles',\n",
              " \"quelqu'un\",\n",
              " 'quelque',\n",
              " 'quelques',\n",
              " 'quels',\n",
              " 'qui',\n",
              " 'quiconque',\n",
              " 'quinze',\n",
              " 'quoi',\n",
              " 'quoique',\n",
              " 'qu’',\n",
              " 'rare',\n",
              " 'rarement',\n",
              " 'rares',\n",
              " 'relative',\n",
              " 'relativement',\n",
              " 'remarquable',\n",
              " 'rend',\n",
              " 'rendre',\n",
              " 'restant',\n",
              " 'reste',\n",
              " 'restent',\n",
              " 'restrictif',\n",
              " 'retour',\n",
              " 'revoici',\n",
              " 'revoilà',\n",
              " 'rien',\n",
              " \"s'\",\n",
              " 'sa',\n",
              " 'sacrebleu',\n",
              " 'sait',\n",
              " 'sans',\n",
              " 'sapristi',\n",
              " 'sauf',\n",
              " 'se',\n",
              " 'sein',\n",
              " 'seize',\n",
              " 'selon',\n",
              " 'semblable',\n",
              " 'semblaient',\n",
              " 'semble',\n",
              " 'semblent',\n",
              " 'sent',\n",
              " 'sept',\n",
              " 'septième',\n",
              " 'sera',\n",
              " 'seraient',\n",
              " 'serait',\n",
              " 'seront',\n",
              " 'ses',\n",
              " 'seul',\n",
              " 'seule',\n",
              " 'seulement',\n",
              " 'si',\n",
              " 'sien',\n",
              " 'sienne',\n",
              " 'siennes',\n",
              " 'siens',\n",
              " 'sinon',\n",
              " 'six',\n",
              " 'sixième',\n",
              " 'soi',\n",
              " 'soi-même',\n",
              " 'soit',\n",
              " 'soixante',\n",
              " 'son',\n",
              " 'sont',\n",
              " 'sous',\n",
              " 'souvent',\n",
              " 'specifique',\n",
              " 'specifiques',\n",
              " 'speculatif',\n",
              " 'stop',\n",
              " 'strictement',\n",
              " 'subtiles',\n",
              " 'suffisant',\n",
              " 'suffisante',\n",
              " 'suffit',\n",
              " 'suis',\n",
              " 'suit',\n",
              " 'suivant',\n",
              " 'suivante',\n",
              " 'suivantes',\n",
              " 'suivants',\n",
              " 'suivre',\n",
              " 'superpose',\n",
              " 'sur',\n",
              " 'surtout',\n",
              " 's’',\n",
              " \"t'\",\n",
              " 'ta',\n",
              " 'tac',\n",
              " 'tant',\n",
              " 'tardive',\n",
              " 'te',\n",
              " 'tel',\n",
              " 'telle',\n",
              " 'tellement',\n",
              " 'telles',\n",
              " 'tels',\n",
              " 'tenant',\n",
              " 'tend',\n",
              " 'tenir',\n",
              " 'tente',\n",
              " 'tes',\n",
              " 'tic',\n",
              " 'tien',\n",
              " 'tienne',\n",
              " 'tiennes',\n",
              " 'tiens',\n",
              " 'toc',\n",
              " 'toi',\n",
              " 'toi-même',\n",
              " 'ton',\n",
              " 'touchant',\n",
              " 'toujours',\n",
              " 'tous',\n",
              " 'tout',\n",
              " 'toute',\n",
              " 'toutefois',\n",
              " 'toutes',\n",
              " 'treize',\n",
              " 'trente',\n",
              " 'tres',\n",
              " 'trois',\n",
              " 'troisième',\n",
              " 'troisièmement',\n",
              " 'trop',\n",
              " 'très',\n",
              " 'tsoin',\n",
              " 'tsouin',\n",
              " 'tu',\n",
              " 'té',\n",
              " 't’',\n",
              " 'un',\n",
              " 'une',\n",
              " 'unes',\n",
              " 'uniformement',\n",
              " 'unique',\n",
              " 'uniques',\n",
              " 'uns',\n",
              " 'va',\n",
              " 'vais',\n",
              " 'vas',\n",
              " 'vers',\n",
              " 'via',\n",
              " 'vif',\n",
              " 'vifs',\n",
              " 'vingt',\n",
              " 'vivat',\n",
              " 'vive',\n",
              " 'vives',\n",
              " 'vlan',\n",
              " 'voici',\n",
              " 'voilà',\n",
              " 'vont',\n",
              " 'vos',\n",
              " 'votre',\n",
              " 'vous',\n",
              " 'vous-mêmes',\n",
              " 'vu',\n",
              " 'vé',\n",
              " 'vôtre',\n",
              " 'vôtres',\n",
              " 'zut',\n",
              " 'à',\n",
              " 'â',\n",
              " 'ça',\n",
              " 'ès',\n",
              " 'étaient',\n",
              " 'étais',\n",
              " 'était',\n",
              " 'étant',\n",
              " 'été',\n",
              " 'être',\n",
              " 'ô'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    }
  ]
}