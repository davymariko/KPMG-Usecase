{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "docs_labeling_fr.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPBntdtFmG/pt66xNnL7aaY"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kuKWsJV27Xe2"
      },
      "source": [
        "# Importing and preprocessing of data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OHoCSdHswimk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d2aa327-646b-42c5-d3c9-cd72130dff05"
      },
      "source": [
        "from google.colab import drive\r\n",
        "\r\n",
        "drive.mount('/content/drive/')\r\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMVNjvlExvOf"
      },
      "source": [
        "TXTS_PATH = '/content/drive/My Drive/Becode/Kpmg/selection_fr'"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AxwxF21YxnUj"
      },
      "source": [
        "import os"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JZx91aOoyi1d"
      },
      "source": [
        "import pandas as pd\r\n",
        "pd.set_option('display.max_colwidth', 255)"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J_t3pBX-xF7U"
      },
      "source": [
        "KPMG_PATH = '/content/drive/My Drive/Becode/Kpmg'\r\n",
        "RESPONSES_PATH = '/content/drive/My Drive/Becode/Kpmg/responses_2018_now.json'"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DkiARh2Oxg4o"
      },
      "source": [
        "txt_filenames = [f for f in os.listdir(TXTS_PATH) if f.endswith(\".txt\")]"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZddQl2FVydlE"
      },
      "source": [
        "#reading json file to retrieve tags\r\n",
        "responses = pd.read_json(RESPONSES_PATH)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "ycPxUepwyxtL",
        "outputId": "38e38403-9645-4094-be3d-d4a73b182a8c"
      },
      "source": [
        "#filtering only relevant columns\r\n",
        "columns = ['jcId', 'jcFr', 'titleFr', 'themesFr', 'scopeFr', 'noScopeFr', 'documentLink']\r\n",
        "responses2 = responses.copy(deep=True).loc[:,columns]\r\n",
        "#getting txt_name from original pdf name\r\n",
        "responses2[\"txt_name\"] = responses2[\"documentLink\"]\r\n",
        "responses2[\"txt_name\"] = responses2[\"txt_name\"].str.replace(\"/\",\"-\")\r\n",
        "responses2[\"txt_name\"] = responses2[\"txt_name\"].str.replace(\".pdf\",\"_FR.txt\")\r\n",
        "#filtering json only for selected files and columns\r\n",
        "columns += ['txt_name']\r\n",
        "responses2 = responses2.loc[responses2.txt_name.isin(txt_filenames), columns]\r\n",
        "responses2.head(2)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>jcId</th>\n",
              "      <th>jcFr</th>\n",
              "      <th>titleFr</th>\n",
              "      <th>themesFr</th>\n",
              "      <th>scopeFr</th>\n",
              "      <th>noScopeFr</th>\n",
              "      <th>documentLink</th>\n",
              "      <th>txt_name</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>1110000</td>\n",
              "      <td>COMMISSION PARITAIRE DES CONSTRUCTIONS METALLI...</td>\n",
              "      <td>modification du régime de pension sectoriel so...</td>\n",
              "      <td>[PENSIONS COMPÉMENTAIRES ET ASSURANCES GROUPES]</td>\n",
              "      <td>None</td>\n",
              "      <td>[les employeurs et ouvriers des entreprises ex...</td>\n",
              "      <td>111/111-2018-013525.pdf</td>\n",
              "      <td>111-111-2018-013525_FR.txt</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>1110000</td>\n",
              "      <td>COMMISSION PARITAIRE DES CONSTRUCTIONS METALLI...</td>\n",
              "      <td>allocation spéciale compensatoire</td>\n",
              "      <td>[PRIME SYNDICALE]</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>111/111-2018-012196.pdf</td>\n",
              "      <td>111-111-2018-012196_FR.txt</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       jcId  ...                    txt_name\n",
              "34  1110000  ...  111-111-2018-013525_FR.txt\n",
              "35  1110000  ...  111-111-2018-012196_FR.txt\n",
              "\n",
              "[2 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XlXwURIgy2uP",
        "outputId": "a110bf5f-a0ad-41ad-8854-93f9751ececc"
      },
      "source": [
        "#generating all possible FR themes\r\n",
        "themes_fr = []\r\n",
        "for r in  responses2.themesFr:\r\n",
        "    if r is not None:\r\n",
        "        for t in r:\r\n",
        "            if t not in themes_fr:\r\n",
        "                themes_fr += [t]\r\n",
        "len(themes_fr)\r\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "54"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r2kAiO8Uy9Eu"
      },
      "source": [
        "#code to retrieve the entire body\r\n",
        "#%%\r\n",
        "for filename in responses2.txt_name.to_list():\r\n",
        "    # filename = responses2.txt_name.to_list()[10] #test\r\n",
        "    file_path = os.path.join(TXTS_PATH, filename)\r\n",
        "    with open(file_path, 'r', encoding=\"utf8\") as f:\r\n",
        "        #to retrieve entire body not necessary\r\n",
        "        responses2.loc[responses2.txt_name == filename, \"doc_bodies\"] = f.read()\r\n",
        "        f.close()"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iox0MB-33HOD"
      },
      "source": [
        "t = themes_fr[0]\r\n",
        "mask = responses2.dropna(axis=0, subset=['themesFr']).themesFr.map(lambda x: t in x)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NUAfSmIf16p-",
        "outputId": "7e6c1eb9-fa19-40b7-cd6d-27a6299dec23"
      },
      "source": [
        "len(responses2)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "578"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0kHR6RAx2OK0"
      },
      "source": [
        " #len(responses2.dropna(axis=0, subset=['themesFr'])[map])"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Zv_1-VwyT4s"
      },
      "source": [
        "# Regex extraction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8rzHtkeWyOdQ"
      },
      "source": [
        "\"\"\"This script regex-extracts information from the document bodies to new DataFrame columns\"\"\"\r\n",
        "#redundant libraries to be checked\r\n",
        "import datetime\r\n",
        "from pickle import load\r\n",
        "import re\r\n",
        "\r\n",
        "import numpy as np\r\n",
        "import pandas as pd"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rq9BZQMsyjQc"
      },
      "source": [
        "def add_scope(df, body_column:str):\r\n",
        "\r\n",
        "\tdef split_func(text):\r\n",
        "\t\ttexts_lst = []\r\n",
        "\t\tsents = text.split(\"\\n\\n\")\r\n",
        "\t\tfor sent in sents:\r\n",
        "\t\t\ttexts_lst.append(sent)\r\n",
        "\t\treturn texts_lst\r\n",
        "\r\n",
        "\tdef extract_scope(lst):\r\n",
        "\t\tscope_lst = []\r\n",
        "\t\tfor item in lst:\r\n",
        "\t\t\t#pattern1 = r\".*\\b(convention collective de travail s'applique | Champ d'application)\\b.*\"\r\n",
        "\t\t\t#pattern2 = r\".*\\b(La présente convention collective de travail\\s*\\S* [a-z]'appli\\w*)\\b.*\"\r\n",
        "\t\t\t#pattern3 = r\".*\\b(convention collective de travail|CCT)\\s*\\S*[a-z]('appli\\w*)\\b.*\"\r\n",
        "\t\t\t#pattern4 = r\".*\\b(convention collective de travail|CCT)\\s*\\S*.*('?appli\\w*)\\b.*\"\r\n",
        "\t\t\tpattern = r\".*\\b(convention collective de.*travail|CCT).*('*appli\\w*)\\b.*\"\r\n",
        "\t\t\tif re.search(pattern, item, flags=re.IGNORECASE):\r\n",
        "\t\t\t\tscope_lst.append(item)\r\n",
        "\t\t\telse:\r\n",
        "\t\t\t\tpass\r\n",
        "\r\n",
        "  \r\n",
        "\tdef extract_depot(lst):\r\n",
        "\t\tdepot_lst = []\r\n",
        "\t\tfor item in lst:\r\n",
        "\t\t\tpattern = r\".*(uméro de dépôt:).*\"\r\n",
        "\t\t\tif re.search(pattern, item, flags=re.IGNORECASE):\r\n",
        "\t\t\t\tdepot_lst.append(item)\r\n",
        "\t\t\telse:\r\n",
        "\t\t\t\tpass\r\n",
        "\t\t# DataFrame convention is to have scope as a string\r\n",
        "\t\treturn '\\n'.join(depot_lst)\r\n",
        "\r\n",
        "\tdf['texts_lst'] = df[body_column].apply(split_func)\r\n",
        "\tdf['scopeFr'] = df['texts_lst'].apply(extract_scope)\r\n",
        "\tdf['depotFr'] = df['texts_lst'].apply(extract_depot)\r\n",
        "  #df.drop(columns=['texts_lst'], inplace = True)\r\n",
        "\r\n",
        "\treturn df"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1BB9v37fy5Gc",
        "outputId": "79258b2c-4230-49e5-c66a-1c5f5277449d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "responses2_5 = add_scope(responses2.head(5), 'doc_bodies')"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:35: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:37: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xx8mwK9VzojZ",
        "outputId": "65ded30a-7458-40f9-e9e6-2e8187728c59",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "responses2_5.depotFr"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "34    \n",
              "35    \n",
              "36    \n",
              "37    \n",
              "38    \n",
              "Name: depotFr, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o8soB9DV0imI",
        "outputId": "dd7bca9c-2f70-4a47-fb04-2a1c4d08aba0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "responses2.scopeFr[:5]"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "34                                                                                                                                                                                                                                                                  \n",
              "35                               Article 1er. - La présente convention collective de travail s'applique aux employeurs, aux ouvriers et aux ouvrières des entreprises ressortissant à la Commission paritaire des constructions métallique, mécanique et électrique.\n",
              "36    La présente convention collective de travail s'applique aux employeurs et aux ouvriers des entreprises de montage de ponts et de charpentes métalliques ressortissant à la Commission paritaire des constructions métallique, mécanique et électrique, à l'...\n",
              "37    Article 1”. La convention collective de travail s'applique aux employeurs et ouvriers (h/f) des entreprises établies dans la province d'Anvers ressortissant à la Commission paritaire 111 pour les ouvriers des constructions métallique, mécanique et éle...\n",
              "38    La présente convention collective de travail s'applique aux employeurs et aux ouvriers des entreprises qui ressortissent à la Commission paritaire des constructions métallique, mécanique et électrique, à l'exception des entreprises de montage de ponts...\n",
              "Name: scopeFr, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qWn_16ThyokX"
      },
      "source": [
        "\r\n",
        "def add_effective_date(df):\r\n",
        "    \"\"\"A function that searches the document bodies for fromDate and endDates\r\n",
        "    and extracts datetime objects into new columns\"\"\"\r\n",
        "    \r\n",
        "    def get_dates(df_row):\r\n",
        "        \r\n",
        "        def get_month_digit(month):\r\n",
        "            dictionary = {\r\n",
        "                'janvier': 1,\r\n",
        "                'février': 2,\r\n",
        "                'mars': 3,\r\n",
        "                'avril': 4,\r\n",
        "                'mai': 5,\r\n",
        "                'juin': 6,\r\n",
        "                'juillet': 7,\r\n",
        "                'août': 8,\r\n",
        "                'septembre': 9,\r\n",
        "                'octobre': 10,\r\n",
        "                'novembre': 11,\r\n",
        "                'décembre': 12\r\n",
        "            }\r\n",
        "            \r\n",
        "            return dictionary[month]\r\n",
        "        \r\n",
        "        #start date matches ~46 out of 48 test cases\r\n",
        "        #group 6 = month\r\n",
        "        #group 7 = year\r\n",
        "        pattern = r\"((cette|la présente) (convention collective de travail|CCT)|elle).+(entre en vigueur|à partir|produit ses effets|s'étend|sort ses effets|prend cours|\\bdéterminée|allant).{1,19}(le\\b|du\\b|de\\b|au\\b).{1,5}(janvier|février|mars|avril|mai|juin|juillet|août|septembre|octobre|novembre|décembre) ([0-9]{4})\"\r\n",
        "        # finds the first instance of the regex in the body\r\n",
        "        match = re.search(pattern, df_row['docBodyFr'], flags=re.IGNORECASE)\r\n",
        "        if match:\r\n",
        "            try:\r\n",
        "                df_row['fromDate'] = datetime.date(int(match[7]), get_month_digit(match[6]), 1)\r\n",
        "            except:\r\n",
        "                pass\r\n",
        "        else:\r\n",
        "            df_row['fromDate'] = np.nan\r\n",
        "        \r\n",
        "        #end date matches 47 of 48\r\n",
        "        #group 6 = month\r\n",
        "        #group 7 = year\r\n",
        "        pattern = r\"((cette|la présente) (convention collective de travail|CCT)|elle).+([0-9]{4}|cesse de produire ses effets|cesse d'être en vigueur|cesse ses effets|prend fin|expire|conclue jusq|prend.{1,25}fin).+(le\\b|au\\b).{1,5}(janvier|février|mars|avril|mai|juin|juillet|août|septembre|octobre|novembre|décembre) ([0-9]{4})\"\r\n",
        "        match = re.search(pattern, df_row['docBodyFr'], flags=re.IGNORECASE)\r\n",
        "        if match:\r\n",
        "            try:\r\n",
        "                df_row['endDate'] = datetime.date(int(match[7]), get_month_digit(match[6]), 28)\r\n",
        "            except:\r\n",
        "                pass\r\n",
        "        else:\r\n",
        "            df_row['endDate'] = np.nan\r\n",
        "        \r\n",
        "        #there might have been an erroneous match of endDate\r\n",
        "        #which should be overwritten by None when durée idéterminée is detected\r\n",
        "        pattern = r\"((cette|la présente) (convention collective de travail|CCT)|elle).+(durée indéterminée|([0-9]{4}.+à l'exception))\"\r\n",
        "        match = re.search(pattern, df_row['docBodyFr'], flags=re.IGNORECASE)\r\n",
        "        if match:\r\n",
        "            df_row['endDate'] = np.nan\r\n",
        "        \r\n",
        "        return df_row\r\n",
        "    \r\n",
        "    df = df.apply(get_dates, axis=1)\r\n",
        "\r\n",
        "    return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-eyc6d2ByrAn"
      },
      "source": [
        "def extract(df):\r\n",
        "\t'''overall regex extraction functioin'''\r\n",
        "\r\n",
        "\tdf = add_scope(df)\r\n",
        "\r\n",
        "\tdf = add_effective_date(df)\r\n",
        "\r\n",
        "\treturn df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H2PzphfD67lX"
      },
      "source": [
        "# Preprocessing for NLP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R1EBJ4N3TaKL"
      },
      "source": [
        "from pandas import Series\r\n",
        "\r\n",
        "from nltk.corpus import wordnet\r\n",
        "from nltk.tokenize import word_tokenize\r\n",
        "from nltk.tag import pos_tag\r\n",
        "from nltk.stem import WordNetLemmatizer\r\n",
        "\r\n",
        "# import entirely spacy to create Doc objects through nlp\r\n",
        "import spacy\r\n",
        "from spacy import load, lang\r\n",
        "\r\n",
        "from wordcloud import WordCloud\r\n",
        "\r\n",
        "from collections import Counter\r\n",
        "\r\n",
        "from typing import List\r\n",
        "from typing import Dict\r\n",
        "\r\n",
        "# WARNINGS\r\n",
        "# W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found\r\n",
        "# I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine\r\n",
        "\r\n",
        "# GLOBAL VARIABLES\r\n",
        "NAMED_ENTITIES = ['microsoft']\r\n",
        "\r\n",
        "#testing\r\n",
        "from os import getcwd as cwd\r\n",
        "from os.path import dirname as dir\r\n",
        "from os.path import join\r\n",
        "import pandas as pd"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nC6sZ-BPTcUs"
      },
      "source": [
        "def lemmatize(text_tokens: List[str]) -> List[str]:\r\n",
        "    def get_wordnet_pos(word):\r\n",
        "        \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\r\n",
        "        tag = pos_tag([word])[0][1][0].upper()\r\n",
        "        tag_dict = {\"J\": wordnet.ADJ,\r\n",
        "                    \"N\": wordnet.NOUN,\r\n",
        "                    \"V\": wordnet.VERB,\r\n",
        "                    \"R\": wordnet.ADV}\r\n",
        "        return tag_dict.get(tag, wordnet.NOUN)\r\n",
        "\r\n",
        "    # Instantiate the WordNetLemmatizer\r\n",
        "    wordnet_lemmatizer = WordNetLemmatizer()\r\n",
        "    # Lemmatize all tokens into a new list: lemmatized\r\n",
        "    texts_lemmatized = [wordnet_lemmatizer.lemmatize(t, get_wordnet_pos(t)) for t in text_tokens]\r\n",
        "    return texts_lemmatized\r\n",
        "\r\n",
        "\r\n",
        "def filter_words(texts_lemmatized: List[List[str]], freq_min=None, freq_max=None):\r\n",
        "    frequency_absolute = Counter([item for elem in texts_lemmatized for item in elem])\r\n",
        "    wordcloud = WordCloud(width=1000, height=500).generate_from_frequencies(frequency_absolute)\r\n",
        "    frequency_relative = wordcloud.words_\r\n",
        "    if freq_min is not None and freq_min > 0 and freq_min < 1:\r\n",
        "        rel_freq_filtered = {k: v for k, v in frequency_relative.items() if v > freq_min}\r\n",
        "    if freq_max is not None and freq_max > 0 and freq_max < 1:\r\n",
        "        rel_freq_filtered = {k: v for k, v in frequency_relative.items() if v < freq_max}\r\n",
        "    texts_filtered = [[t for t in pub_lem if t in rel_freq_filtered.keys()] for pub_lem in texts_lemmatized]\r\n",
        "    # testing part\r\n",
        "    f_abs_updated =Counter([ item for elem in texts_filtered for item in elem])   \r\n",
        "    wordcloud_updated = WordCloud(width=1000, height=500).generate_from_frequencies(f_abs_updated)\r\n",
        "    f_rel_updated = wordcloud_updated.words_\r\n",
        "    return texts_filtered, f_abs_updated, f_rel_updated, wordcloud_updated\r\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ib8xc7-F5tGY"
      },
      "source": [
        "class Preprocess:\r\n",
        "    def __init__(self, nlp_model='en_core_web_md'):\r\n",
        "        self.nlp = load(nlp_model)\r\n",
        "        if 'en_' in nlp_model:\r\n",
        "          self.stop_words = lang.en.stop_words.STOP_WORDS\r\n",
        "        elif 'fr_' in nlp_model:\r\n",
        "          self.stop_words = lang.fr.stop_words.STOP_WORDS\r\n",
        "        self.named_entities = set(NAMED_ENTITIES)\r\n",
        "\r\n",
        "    def get_named_entities(self, texts: Series, inplace=True) -> set:\r\n",
        "        # creating a single ner set\r\n",
        "        nes = set()\r\n",
        "        # function to extract NER from text\r\n",
        "        def get_named_entities(text) -> set:\r\n",
        "            doc = self.nlp(text)\r\n",
        "            named_entities = set([ent.text for ent in doc.ents])\r\n",
        "            return named_entities\r\n",
        "        [[nes.add(n) for n in get_named_entities(text)] for text in texts]\r\n",
        "        # adding predefined NER\r\n",
        "        [nes.add(n) for n in self.named_entities]\r\n",
        "        if inplace:\r\n",
        "            self.named_entities = nes\r\n",
        "        return nes\r\n",
        "\r\n",
        "    def tokenize_text(self, text:str, stop_words: List[str] = None, named_entities: List[str] = None,\r\n",
        "                   lenght_min: int=2) -> List[str]:\r\n",
        "        if stop_words is None:\r\n",
        "            stop_words = self.stop_words\r\n",
        "        if named_entities is None:\r\n",
        "            named_entities = self.named_entities\r\n",
        "        text = text.replace(\"\\n\", \" \")\r\n",
        "        # split string into words (tokens)\r\n",
        "        tokens = word_tokenize(text.lower())\r\n",
        "        # keep strings with only alphabets\r\n",
        "        tokens = [t for t in tokens if t.isalpha()]\r\n",
        "        tokens = lemmatize(tokens)\r\n",
        "        # remove short words, they're probably not useful\r\n",
        "        tokens = [t for t in tokens if len(t) > lenght_min]\r\n",
        "        # remove stopwords\r\n",
        "        tokens = [t for t in tokens if t not in stop_words]\r\n",
        "        # remove\r\n",
        "        tokens = [t for t in tokens if t not in named_entities]\r\n",
        "        return tokens\r\n",
        "\r\n",
        "    def clean_text(self, text:str, stop_words: List[str] = None, named_entities: List[str] = None,\r\n",
        "                   lenght_min: int=2) -> str:\r\n",
        "        tokens = self.tokenize_text(text, stop_words, named_entities, lenght_min)\r\n",
        "        text_cleaned = \" \".join(tokens)\r\n",
        "        return text_cleaned\r\n",
        "\r\n",
        "    def tokenize_texts(self, texts:Series, stop_words: List[str] = None, named_entities: List[str] = None,\r\n",
        "                   lenght_min: int=2) -> List[List[str]]:\r\n",
        "        texts_tokens = []\r\n",
        "        for text in texts:\r\n",
        "            texts_tokens += [self.tokenize_text(text, stop_words, named_entities, lenght_min)]\r\n",
        "        return texts_tokens"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UzM0CO3T7Jp5",
        "outputId": "ee804e12-e8af-456a-ffaa-57eb05e1cefb"
      },
      "source": [
        "! python -m spacy download fr_core_news_md"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: fr_core_news_md==2.2.5 from https://github.com/explosion/spacy-models/releases/download/fr_core_news_md-2.2.5/fr_core_news_md-2.2.5.tar.gz#egg=fr_core_news_md==2.2.5 in /usr/local/lib/python3.7/dist-packages (2.2.5)\n",
            "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from fr_core_news_md==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->fr_core_news_md==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->fr_core_news_md==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->fr_core_news_md==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->fr_core_news_md==2.2.5) (54.0.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->fr_core_news_md==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->fr_core_news_md==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->fr_core_news_md==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->fr_core_news_md==2.2.5) (1.19.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->fr_core_news_md==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->fr_core_news_md==2.2.5) (2.0.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->fr_core_news_md==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->fr_core_news_md==2.2.5) (3.0.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->fr_core_news_md==2.2.5) (0.8.2)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->fr_core_news_md==2.2.5) (3.7.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->fr_core_news_md==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->fr_core_news_md==2.2.5) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->fr_core_news_md==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->fr_core_news_md==2.2.5) (2020.12.5)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->fr_core_news_md==2.2.5) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->fr_core_news_md==2.2.5) (3.4.1)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('fr_core_news_md')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-gSiOJ547ivj"
      },
      "source": [
        "import spacy\r\n",
        "nlp = spacy.load('fr_core_news_md')"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m7pS6KZYHixh"
      },
      "source": [
        "preprocess = Preprocess(nlp_model='fr_core_news_md')"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dZ1u8dKqIOi2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31d0d630-0006-4c74-df59-db3ecbf9eabc"
      },
      "source": [
        "# example of FR stopwords from the set\r\n",
        "list(preprocess.stop_words)[:10]"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['eu',\n",
              " 'mien',\n",
              " 'pres',\n",
              " \"c'\",\n",
              " 'peuvent',\n",
              " 'plusieurs',\n",
              " 'sienne',\n",
              " 'ah',\n",
              " 'quatrièmement',\n",
              " 'houp']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ZKK_K-6O8Dh",
        "outputId": "9e4dd88a-d9ea-4932-b486-72f50f02ba6e"
      },
      "source": [
        "# importing nltk and downloading additional required packages\r\n",
        "import nltk\r\n",
        "nltk.download('punkt')\r\n",
        "nltk.download('averaged_perceptron_tagger')\r\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nlVBC4-mOu4K"
      },
      "source": [
        "#cleaning text before extracting word clouds\r\n",
        "responses2[\"bodies_cleaned\"] = responses2[\"doc_bodies\"].apply(preprocess.clean_text)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gfpoq2YLL4na"
      },
      "source": [
        "# Word cloud labelling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qsmt70TiL7FV",
        "outputId": "0a09a961-abdf-479f-80a1-3449c6bb4e68",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        }
      },
      "source": [
        "#extracting all words with relative frequency > 0.5\r\n",
        "all_ts = preprocess.tokenize_texts(responses2.bodies_cleaned)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-dd28f965f7d5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#extracting all words with relative frequency > 0.5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mall_ts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize_texts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponses2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbodies_cleaned\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-16-7573f780783e>\u001b[0m in \u001b[0;36mtokenize_texts\u001b[0;34m(self, texts, stop_words, named_entities, lenght_min)\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mtexts_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtexts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m             \u001b[0mtexts_tokens\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnamed_entities\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlenght_min\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtexts_tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-7573f780783e>\u001b[0m in \u001b[0;36mtokenize_text\u001b[0;34m(self, text, stop_words, named_entities, lenght_min)\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;31m# keep strings with only alphabets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misalpha\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlemmatize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0;31m# remove short words, they're probably not useful\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mlenght_min\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-18f0b9ba023e>\u001b[0m in \u001b[0;36mlemmatize\u001b[0;34m(text_tokens)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mwordnet_lemmatizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWordNetLemmatizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m# Lemmatize all tokens into a new list: lemmatized\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mtexts_lemmatized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mwordnet_lemmatizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_wordnet_pos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtext_tokens\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtexts_lemmatized\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-18f0b9ba023e>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mwordnet_lemmatizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWordNetLemmatizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m# Lemmatize all tokens into a new list: lemmatized\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mtexts_lemmatized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mwordnet_lemmatizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_wordnet_pos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtext_tokens\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtexts_lemmatized\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-18f0b9ba023e>\u001b[0m in \u001b[0;36mget_wordnet_pos\u001b[0;34m(word)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_wordnet_pos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0;34m\"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mtag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpos_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         tag_dict = {\"J\": wordnet.ADJ,\n\u001b[1;32m      6\u001b[0m                     \u001b[0;34m\"N\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mwordnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNOUN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/tag/__init__.py\u001b[0m in \u001b[0;36mpos_tag\u001b[0;34m(tokens, tagset, lang)\u001b[0m\n\u001b[1;32m    132\u001b[0m     \"\"\"\n\u001b[1;32m    133\u001b[0m     \u001b[0mtagger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_tagger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_pos_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtagset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtagger\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/tag/__init__.py\u001b[0m in \u001b[0;36m_pos_tag\u001b[0;34m(tokens, tagset, tagger)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_pos_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtagset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtagger\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m     \u001b[0mtagged_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtagger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtagset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0mtagged_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'en-ptb'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtagset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtag\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtagged_tokens\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/tag/perceptron.py\u001b[0m in \u001b[0;36mtag\u001b[0;34m(self, tokens)\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtag\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m                 \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprev2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m                 \u001b[0mtag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m             \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m             \u001b[0mprev2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprev\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/tag/perceptron.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, features)\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeat\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m                 \u001b[0mscores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m         \u001b[0;31m# Do a secondary alphabetic sort, for stability\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQ4gk4WKTHMO"
      },
      "source": [
        "texts, f_abs, f_rel, wordcloud = filter_words(all_ts, freq_min=0.5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0tNIVnCaTmVs"
      },
      "source": [
        "wordcloud.to_image()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EilnPDhRTs9z"
      },
      "source": [
        "# adding most frequent words as stop words\r\n",
        "[preprocess.stop_words.add(w) for w in list(f_rel.keys())]\r\n",
        "[w for w in list(preprocess.stop_words) if w in list(f_rel.keys())]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hiB25mXkUjIG"
      },
      "source": [
        "[w for w in list(preprocess.stop_words) if w in list(f_rel.keys())]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XtLYJqkRXJOm"
      },
      "source": [
        "themes_keywords = pd.DataFrame(columns=['themeFr', 'frequency_absolute', 'frequency_relative'])\r\n",
        "for t in themes_fr:\r\n",
        "  mask = responses2.dropna(axis=0, subset=['themesFr']).themesFr.map(lambda x: t in x)\r\n",
        "  r_t = responses2.dropna(axis=0, subset=['themesFr'])[mask]\r\n",
        "  t_ts = preprocess.tokenize_texts(r_t[\"bodies_cleaned\"])\r\n",
        "  t_texts, t_f_abs, t_f_rel, t_wordcloud = filter_words(t_ts, freq_min=0.05, freq_max=0.95)\r\n",
        "  themes_keywords = themes_keywords.append({'themeFr':t, 'frequency_absolute':t_f_abs, 'frequency_relative':t_f_rel}, ignore_index=True)\r\n",
        "  print(f'Kewyords for {t} added, {len(r_t)} documents found')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dLtvqXPWgKgH"
      },
      "source": [
        "for t in themes_fr:\r\n",
        "  mask = responses2.dropna(axis=0, subset=['themesFr']).themesFr.map(lambda x: t in x)\r\n",
        "  r_t = responses2.dropna(axis=0, subset=['themesFr'])[mask]\r\n",
        "  print(f'{len(r_t)} contained for {t}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "33e2Jk09f74p"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pqhn8BJPYocl"
      },
      "source": [
        "themes_keywords.to_csv('/content/drive/My Drive/Becode/Kpmg/labeling.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJpQLbYqZk74"
      },
      "source": [
        "themes_keywords.loc[:, ['themeFr','frequency_relative']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SpotjOgBmLpS"
      },
      "source": [
        "# Classification model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xGZ0nUEXmd2j"
      },
      "source": [
        "from pickle import load\r\n",
        "\r\n",
        "with open(os.path.join(KPMG_PATH, \"clas_new.pkl\"), 'rb') as f:\r\n",
        "  df = load(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n62CpQau9j-Y"
      },
      "source": [
        "df.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8zKReQwv9xc9"
      },
      "source": [
        "df[\"body_cleaned\"] = df[\"docBodyFr\"].apply(preprocess.clean_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SF6EgPpW-cGJ"
      },
      "source": [
        "#generating all possible FR themes (replacing previous list of 53 items)\r\n",
        "themes_fr = []\r\n",
        "for r in  df.themesFr:\r\n",
        "    if r is not None:\r\n",
        "        for t in r:\r\n",
        "            if t not in themes_fr:\r\n",
        "                themes_fr += [t]\r\n",
        "len(themes_fr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-IrpIu9a_IbD"
      },
      "source": [
        "# showing only 27 unique themes against 54 available\r\n",
        "df.dropna(axis=0, subset=['themesFr'], inplace=True)\r\n",
        "df['theme'] = df['themesFr'].apply(lambda x: x[0] if len(x) == 1 else None)\r\n",
        "len(df.theme.unique())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WMO4woG9JDnc"
      },
      "source": [
        "for t in themes_fr:\r\n",
        "\tdf.dropna(axis=0, subset=['themesFr'], inplace=True)\r\n",
        "\tdf[t] = df['themesFr'].apply(lambda x: 1 if t in x else 0)\r\n",
        "df.columns"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}