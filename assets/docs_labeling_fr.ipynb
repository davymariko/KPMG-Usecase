{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "docs_labeling_fr.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOU7JxK3KGhbl9mhH9ABrAv"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kuKWsJV27Xe2"
      },
      "source": [
        "# Importing and preprocessing of data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OHoCSdHswimk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71a73335-e61e-48fa-f6d4-cf23e93b9a94"
      },
      "source": [
        "from google.colab import drive\r\n",
        "\r\n",
        "drive.mount('/content/drive/')\r\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMVNjvlExvOf"
      },
      "source": [
        "TXTS_PATH = '/content/drive/My Drive/Becode/Kpmg/selection_fr'"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AxwxF21YxnUj"
      },
      "source": [
        "import os"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JZx91aOoyi1d"
      },
      "source": [
        "import pandas as pd\r\n",
        "pd.set_option('display.max_colwidth', 255)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J_t3pBX-xF7U"
      },
      "source": [
        "KPMG_PATH = '/content/drive/My Drive/Becode/Kpmg'\r\n",
        "RESPONSES_PATH = '/content/drive/My Drive/Becode/Kpmg/responses_2018_now.json'"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DkiARh2Oxg4o"
      },
      "source": [
        "txt_filenames = [f for f in os.listdir(TXTS_PATH) if f.endswith(\".txt\")]"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZddQl2FVydlE"
      },
      "source": [
        "#reading json file to retrieve tags\r\n",
        "responses = pd.read_json(RESPONSES_PATH)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "id": "ycPxUepwyxtL",
        "outputId": "e7aa72b7-5a87-4a61-d6f1-ce9c1901654b"
      },
      "source": [
        "#filtering only relevant columns\r\n",
        "columns = ['jcId', 'jcFr', 'titleFr', 'themesFr', 'scopeFr', 'noScopeFr', 'documentLink']\r\n",
        "responses2 = responses.copy(deep=True).loc[:,columns]\r\n",
        "#getting txt_name from original pdf name\r\n",
        "responses2[\"txt_name\"] = responses2[\"documentLink\"]\r\n",
        "responses2[\"txt_name\"] = responses2[\"txt_name\"].str.replace(\"/\",\"-\")\r\n",
        "responses2[\"txt_name\"] = responses2[\"txt_name\"].str.replace(\".pdf\",\"_FR.txt\")\r\n",
        "#filtering json only for selected files and columns\r\n",
        "columns += ['txt_name']\r\n",
        "responses2 = responses2.loc[responses2.txt_name.isin(txt_filenames), columns]\r\n",
        "responses2.head(2)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>jcId</th>\n",
              "      <th>jcFr</th>\n",
              "      <th>titleFr</th>\n",
              "      <th>themesFr</th>\n",
              "      <th>scopeFr</th>\n",
              "      <th>noScopeFr</th>\n",
              "      <th>documentLink</th>\n",
              "      <th>txt_name</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>1110000</td>\n",
              "      <td>COMMISSION PARITAIRE DES CONSTRUCTIONS METALLIQUE, MECANIQUE ET ELECTRIQUE</td>\n",
              "      <td>modification du régime de pension sectoriel social, du règlement de pension et du règlement de solidarité</td>\n",
              "      <td>[PENSIONS COMPÉMENTAIRES ET ASSURANCES GROUPES]</td>\n",
              "      <td>None</td>\n",
              "      <td>[les employeurs et ouvriers des entreprises exemptées du paiement d'une cotisation pour le régime de pension sectorielsur la base d'un accord d'entreprise relatif à l'instauration ou à l'élargissement d'un régime de pension complémentaire, employeurs ...</td>\n",
              "      <td>111/111-2018-013525.pdf</td>\n",
              "      <td>111-111-2018-013525_FR.txt</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>1110000</td>\n",
              "      <td>COMMISSION PARITAIRE DES CONSTRUCTIONS METALLIQUE, MECANIQUE ET ELECTRIQUE</td>\n",
              "      <td>allocation spéciale compensatoire</td>\n",
              "      <td>[PRIME SYNDICALE]</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>111/111-2018-012196.pdf</td>\n",
              "      <td>111-111-2018-012196_FR.txt</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       jcId  ...                    txt_name\n",
              "34  1110000  ...  111-111-2018-013525_FR.txt\n",
              "35  1110000  ...  111-111-2018-012196_FR.txt\n",
              "\n",
              "[2 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XlXwURIgy2uP",
        "outputId": "ee37f5fa-aabf-4a84-b418-5e02ae519797"
      },
      "source": [
        "#generating all possible FR themes\r\n",
        "themes_fr = []\r\n",
        "for r in  responses2.themesFr:\r\n",
        "    if r is not None:\r\n",
        "        for t in r:\r\n",
        "            if t not in themes_fr:\r\n",
        "                themes_fr += [t]\r\n",
        "len(themes_fr)\r\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "54"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r2kAiO8Uy9Eu"
      },
      "source": [
        "#code to retrieve the entire body\r\n",
        "#%%\r\n",
        "for filename in responses2.txt_name.to_list():\r\n",
        "    # filename = responses2.txt_name.to_list()[10] #test\r\n",
        "    file_path = os.path.join(TXTS_PATH, filename)\r\n",
        "    with open(file_path, 'r', encoding=\"utf8\") as f:\r\n",
        "        #to retrieve entire body not necessary\r\n",
        "        responses2.loc[responses2.txt_name == filename, \"doc_bodies\"] = f.read()\r\n",
        "        f.close()"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iox0MB-33HOD"
      },
      "source": [
        "t = themes_fr[0]\r\n",
        "mask = responses2.dropna(axis=0, subset=['themesFr']).themesFr.map(lambda x: t in x)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NUAfSmIf16p-",
        "outputId": "39a1d5fc-05bb-43c2-a383-37b106ae9949"
      },
      "source": [
        "len(responses2)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "578"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0kHR6RAx2OK0"
      },
      "source": [
        " #len(responses2.dropna(axis=0, subset=['themesFr'])[map])"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Zv_1-VwyT4s"
      },
      "source": [
        "# Regex extraction\r\n",
        "[RegEx Match Object](https://www.w3schools.com/python/gloss_python_regex_match.asp)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8rzHtkeWyOdQ"
      },
      "source": [
        "\"\"\"This script regex-extracts information from the document bodies to new DataFrame columns\"\"\"\r\n",
        "#redundant libraries to be checked\r\n",
        "import datetime\r\n",
        "from pickle import load\r\n",
        "import re\r\n",
        "\r\n",
        "import numpy as np\r\n",
        "import pandas as pd"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rq9BZQMsyjQc"
      },
      "source": [
        "def split_func(text):\r\n",
        "  texts_lst = []\r\n",
        "  sents = text.split(\"\\n\\n\")\r\n",
        "  for sent in sents:\r\n",
        "    texts_lst.append(sent)\r\n",
        "  return texts_lst"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pp7kG9t6TgWs"
      },
      "source": [
        "def extract_pattern(lst, pattern:str):\r\n",
        "  output_lst = []\r\n",
        "  for item in lst: \r\n",
        "    if re.search(pattern, item, flags=re.IGNORECASE):\r\n",
        "      output_lst.append(item)\r\n",
        "    else:\r\n",
        "      pass\r\n",
        "  return '\\n'.join(output_lst)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gftt1FgpUG1n"
      },
      "source": [
        "def extract_scope(lst):\r\n",
        "  output_lst = []\r\n",
        "  for item in lst: \r\n",
        "    #pattern1 = r\".*\\b(convention collective de travail s'applique | Champ d'application)\\b.*\"\r\n",
        "    #pattern2 = r\".*\\b(La présente convention collective de travail\\s*\\S* [a-z]'appli\\w*)\\b.*\"\r\n",
        "    #pattern3 = r\".*\\b(convention collective de travail|CCT)\\s*\\S*[a-z]('appli\\w*)\\b.*\"\r\n",
        "    #pattern4 = r\".*\\b(convention collective de travail|CCT)\\s*\\S*.*('?appli\\w*)\\b.*\"\r\n",
        "    #pattern = r\".*\\b(convention collective de.*travail|CCT).*('*appli\\w*)\\b.*\"\r\n",
        "    pattern = r\".*((convention collective de )(travail|CCT))*.*\"\r\n",
        "    if re.search(pattern, item, flags=re.IGNORECASE):\r\n",
        "      output_lst.append(item)\r\n",
        "    else:\r\n",
        "      pass\r\n",
        "  return '\\n'.join(output_lst)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l1uLhwvTUHJC"
      },
      "source": [
        "def extract_deposit(lst):\r\n",
        "  output_lst = []\r\n",
        "  for item in lst: \r\n",
        "    pattern =r\".*(épôt: )([0-9]{2})/([0-9]{2})/([0-9]{4}).*\"\r\n",
        "    match = re.search(pattern, item, flags=re.IGNORECASE)\r\n",
        "    if match:\r\n",
        "      output_str = str(match[2])+\"/\"+str(match[3])+\"/\"+str(match[])\r\n",
        "      output_lst.append(output_str)\r\n",
        "    else:\r\n",
        "      pass\r\n",
        "  return '\\n'.join(output_lst)"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0oH_KeLG-q-q"
      },
      "source": [
        "#Pendant la durée de cette CCT tous les jours de chômage temporaire ne seront pas pris en compte dans le calcul de la période de maximum soixante jours par année de article 2 de la CCT du 17 septembre 2019 conclue au sein de la Commission Paritaire de l’industrie chimique relative à la sécurité d'existence (numéro d'enregistrement 154419).\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QTTnao6puKHt"
      },
      "source": [
        "def add_tags(df, body_column):\r\n",
        "  df['texts_lst'] = df[body_column].apply(split_func)\r\n",
        "  #pattern1 = r\".*\\b(convention collective de travail s'applique | Champ d'application)\\b.*\r\n",
        "  #pattern2 = r\".*\\b(La présente convention collective de travail\\s*\\S* [a-z]'appli\\w*)\\b.*\"\r\n",
        "  #pattern3 = r\".*\\b(convention collective de travail|CCT)\\s*\\S*[a-z]('appli\\w*)\\b.*\"\r\n",
        "  #pattern4 = r\".*\\b(convention collective de travail|CCT)\\s*\\S*.*('?appli\\w*)\\b.*\"\r\n",
        "  #pattern = r\".*\\b(convention collective de.*travail|CCT).*('*appli\\w*)\\b.*\"\r\n",
        "  df['scopeFr'] = df['texts_lst'].apply(extract_scope)\r\n",
        "  df['depotFr'] = df['texts_lst'].apply(extract_deposit)\r\n",
        "  df.drop(columns=['texts_lst'], inplace = True)\r\n",
        "  return df"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DWW3NtM9PxfO",
        "outputId": "a652f3a4-d2b4-48f8-f732-e162c2ce4171"
      },
      "source": [
        "df = responses2.head(5)\r\n",
        "df['texts_lst'] = df[\"doc_bodies\"].apply(split_func)\r\n",
        "df['texts_lst']"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "34    [Neerlegging-Dépôt: 18/12/2018, Regist.-Enregistr.: 07/01/2019, N°: 149887/C0/111 , COMMISSION PARITAIRE DES CONSTRUCTIONS |, METALLIQUE, MECANIQUE ET ELECTRIQUE (PC |, Mt) , | , | , Ì       , CONVENTION COLLECTIVE DE TRAVAIL DU 17, DECEMBRE 2018 MODI...\n",
              "35    [COMMISSION PARITAIRE 111 DES CONSTRUCTIONS, MÉTALLIQUE, MÉCANIQUE ET ÉLECTRIQUE , Convention collective de travail du 15 octobre 2018 , ALLOCATION SPECIALE COMPENSATOIRE , Chapitre ler. - Champ d'application , Article 1er. - La présente convention co...\n",
              "36    [Neerlegging-Dépôt: 17/10/2018, Regist.-Enregistr.: 05/11/2018, N°: 148625/CO/111 , COMMISION PARITAIRE DE LA CONSTRUCTION, METALLIQUE, MECANIQUE ET ELECTRIQUE SECTION MONTAGE (CP 111.3) , Convention collective de travail du 15 octobre 2018 , MODIFICA...\n",
              "37    [Neerlegging-Dépôt: 05/09/2018, Regist.-Enregistr.: 17/10/2018, N°: 148350/C0/111 , Commission paritaire des constructions métallique, mécanique et électrique , Convention collective de travail du 3 septembre 2018 , Addendum à la CCT du 16 octobre 201...\n",
              "38    [Neerlegging-Dépôt: 03/07/2018, Regist.-Enregistr.: 14/08/2018, N°: 147257/CO/111  , COMMISSION PARITAIRE DES CONSTRUCTIONS, METALLIQUE, MECANIQUE ET ELECTRIQUE , Convention collective de travail du 2 juillet 2018, SALAIRE MINIMUM GARANTI , Préambule ...\n",
              "Name: texts_lst, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oGDTnOxJswsV",
        "outputId": "955aee8b-ade4-42ab-b72e-ad253472f230"
      },
      "source": [
        "df['depotFr'] = df['texts_lst'].apply(extract_deposit)\r\n",
        "df['depotFr']\r\n",
        "#df = add_tags(df, \"doc_bodies\")\r\n",
        "#df.loc[:,['txt_name','depotFr']]\r\n"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "34    18/12/2018\n",
              "35              \n",
              "36    17/10/2018\n",
              "37    05/09/2018\n",
              "38    03/07/2018\n",
              "Name: depotFr, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1BB9v37fy5Gc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ae668abd-21f7-4a8b-caba-727c05b1bfd2"
      },
      "source": [
        "df = add_tags(df, 'doc_bodies')\r\n",
        "df"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  if __name__ == '__main__':\n",
            "/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py:4174: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  errors=errors,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>jcId</th>\n",
              "      <th>jcFr</th>\n",
              "      <th>titleFr</th>\n",
              "      <th>themesFr</th>\n",
              "      <th>scopeFr</th>\n",
              "      <th>noScopeFr</th>\n",
              "      <th>documentLink</th>\n",
              "      <th>txt_name</th>\n",
              "      <th>doc_bodies</th>\n",
              "      <th>depotFr</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>1110000</td>\n",
              "      <td>COMMISSION PARITAIRE DES CONSTRUCTIONS METALLIQUE, MECANIQUE ET ELECTRIQUE</td>\n",
              "      <td>modification du régime de pension sectoriel social, du règlement de pension et du règlement de solidarité</td>\n",
              "      <td>[PENSIONS COMPÉMENTAIRES ET ASSURANCES GROUPES]</td>\n",
              "      <td>Neerlegging-Dépôt: 18/12/2018\\nRegist.-Enregistr.: 07/01/2019\\nN°: 149887/C0/111 \\nCOMMISSION PARITAIRE DES CONSTRUCTIONS |\\nMETALLIQUE, MECANIQUE ET ELECTRIQUE (PC |\\nMt) \\n| \\n| \\nÌ       \\nCONVENTION COLLECTIVE DE TRAVAIL DU 17\\nDECEMBRE 2018 MODIF...</td>\n",
              "      <td>[les employeurs et ouvriers des entreprises exemptées du paiement d'une cotisation pour le régime de pension sectorielsur la base d'un accord d'entreprise relatif à l'instauration ou à l'élargissement d'un régime de pension complémentaire, employeurs ...</td>\n",
              "      <td>111/111-2018-013525.pdf</td>\n",
              "      <td>111-111-2018-013525_FR.txt</td>\n",
              "      <td>Neerlegging-Dépôt: 18/12/2018\\n\\nRegist.-Enregistr.: 07/01/2019\\n\\nN°: 149887/C0/111 \\n\\nCOMMISSION PARITAIRE DES CONSTRUCTIONS |\\n\\nMETALLIQUE, MECANIQUE ET ELECTRIQUE (PC |\\n\\nMt) \\n\\n| \\n\\n| \\n\\nÌ       \\n\\nCONVENTION COLLECTIVE DE TRAVAIL DU 17\\n\\...</td>\n",
              "      <td>Neerlegging-Dépôt: 18/12/2018</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>1110000</td>\n",
              "      <td>COMMISSION PARITAIRE DES CONSTRUCTIONS METALLIQUE, MECANIQUE ET ELECTRIQUE</td>\n",
              "      <td>allocation spéciale compensatoire</td>\n",
              "      <td>[PRIME SYNDICALE]</td>\n",
              "      <td>COMMISSION PARITAIRE 111 DES CONSTRUCTIONS\\nMÉTALLIQUE, MÉCANIQUE ET ÉLECTRIQUE \\nConvention collective de travail du 15 octobre 2018 \\nALLOCATION SPECIALE COMPENSATOIRE \\nChapitre ler. - Champ d'application \\nArticle 1er. - La présente convention col...</td>\n",
              "      <td>None</td>\n",
              "      <td>111/111-2018-012196.pdf</td>\n",
              "      <td>111-111-2018-012196_FR.txt</td>\n",
              "      <td>COMMISSION PARITAIRE 111 DES CONSTRUCTIONS\\n\\nMÉTALLIQUE, MÉCANIQUE ET ÉLECTRIQUE \\n\\nConvention collective de travail du 15 octobre 2018 \\n\\nALLOCATION SPECIALE COMPENSATOIRE \\n\\nChapitre ler. - Champ d'application \\n\\nArticle 1er. - La présente conv...</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>1110000</td>\n",
              "      <td>COMMISSION PARITAIRE DES CONSTRUCTIONS METALLIQUE, MECANIQUE ET ELECTRIQUE</td>\n",
              "      <td>modification et coordination de la cct relative aux primes</td>\n",
              "      <td>[TRAVAIL EN ÉQUIPE ET DE NUIT, PÉCULE DE VACANCES, PRIME PROPRE AU SECTEUR OU À L'ENTREPRISE, REMBOURSEMENT DE FRAIS (HORS FRAIS DE DÉPLACEMENT), DÉTACHEMENT]</td>\n",
              "      <td>Neerlegging-Dépôt: 17/10/2018\\nRegist.-Enregistr.: 05/11/2018\\nN°: 148625/CO/111 \\nCOMMISION PARITAIRE DE LA CONSTRUCTION\\nMETALLIQUE, MECANIQUE ET ELECTRIQUE SECTION MONTAGE (CP 111.3) \\nConvention collective de travail du 15 octobre 2018 \\nMODIFICAT...</td>\n",
              "      <td>[secteur des entreprises de fabrications métalliques]</td>\n",
              "      <td>111/111-2018-012197.pdf</td>\n",
              "      <td>111-111-2018-012197_FR.txt</td>\n",
              "      <td>Neerlegging-Dépôt: 17/10/2018\\n\\nRegist.-Enregistr.: 05/11/2018\\n\\nN°: 148625/CO/111 \\n\\nCOMMISION PARITAIRE DE LA CONSTRUCTION\\n\\nMETALLIQUE, MECANIQUE ET ELECTRIQUE SECTION MONTAGE (CP 111.3) \\n\\nConvention collective de travail du 15 octobre 2018 \\...</td>\n",
              "      <td>Neerlegging-Dépôt: 17/10/2018</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>1110000</td>\n",
              "      <td>COMMISSION PARITAIRE DES CONSTRUCTIONS METALLIQUE, MECANIQUE ET ELECTRIQUE</td>\n",
              "      <td>addendum à la cct du 16/10/2017 concernant la prime de fin d'année en province d'Anvers</td>\n",
              "      <td>[PRIME DE FIN D'ANNÉE]</td>\n",
              "      <td>Neerlegging-Dépôt: 05/09/2018\\nRegist.-Enregistr.: 17/10/2018\\nN°: 148350/C0/111 \\nCommission paritaire des constructions métallique, mécanique et électrique \\nConvention collective de travail du 3 septembre 2018 \\nAddendum à la CCT du 16 octobre 2017...</td>\n",
              "      <td>[les entreprises de montage de ponts et charpentes métalliques]</td>\n",
              "      <td>111/111-2018-011427.pdf</td>\n",
              "      <td>111-111-2018-011427_FR.txt</td>\n",
              "      <td>Neerlegging-Dépôt: 05/09/2018\\n\\nRegist.-Enregistr.: 17/10/2018\\n\\nN°: 148350/C0/111 \\n\\nCommission paritaire des constructions métallique, mécanique et électrique \\n\\nConvention collective de travail du 3 septembre 2018 \\n\\nAddendum à la CCT du 16 oc...</td>\n",
              "      <td>Neerlegging-Dépôt: 05/09/2018</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>1110000</td>\n",
              "      <td>COMMISSION PARITAIRE DES CONSTRUCTIONS METALLIQUE, MECANIQUE ET ELECTRIQUE</td>\n",
              "      <td>salaire minimum garanti</td>\n",
              "      <td>[SALAIRES, DÉTACHEMENT]</td>\n",
              "      <td>Neerlegging-Dépôt: 03/07/2018\\nRegist.-Enregistr.: 14/08/2018\\nN°: 147257/CO/111  \\nCOMMISSION PARITAIRE DES CONSTRUCTIONS\\nMETALLIQUE, MECANIQUE ET ELECTRIQUE \\nConvention collective de travail du 2 juillet 2018\\nSALAIRE MINIMUM GARANTI \\nPréambule \\...</td>\n",
              "      <td>[secteur des entreprises de montage de ponts et charpentes métalliques]</td>\n",
              "      <td>111/111-2018-009675.pdf</td>\n",
              "      <td>111-111-2018-009675_FR.txt</td>\n",
              "      <td>Neerlegging-Dépôt: 03/07/2018\\n\\nRegist.-Enregistr.: 14/08/2018\\n\\nN°: 147257/CO/111  \\n\\nCOMMISSION PARITAIRE DES CONSTRUCTIONS\\n\\nMETALLIQUE, MECANIQUE ET ELECTRIQUE \\n\\nConvention collective de travail du 2 juillet 2018\\n\\nSALAIRE MINIMUM GARANTI \\...</td>\n",
              "      <td>Neerlegging-Dépôt: 03/07/2018</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       jcId  ...                        depotFr\n",
              "34  1110000  ...  Neerlegging-Dépôt: 18/12/2018\n",
              "35  1110000  ...                               \n",
              "36  1110000  ...  Neerlegging-Dépôt: 17/10/2018\n",
              "37  1110000  ...  Neerlegging-Dépôt: 05/09/2018\n",
              "38  1110000  ...  Neerlegging-Dépôt: 03/07/2018\n",
              "\n",
              "[5 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MKPgMOZu_Smi"
      },
      "source": [
        "def get_month_digit(month):\r\n",
        "            dictionary = {\r\n",
        "                'janvier': 1,\r\n",
        "                'février': 2,\r\n",
        "                'mars': 3,\r\n",
        "                'avril': 4,\r\n",
        "                'mai': 5,\r\n",
        "                'juin': 6,\r\n",
        "                'juillet': 7,\r\n",
        "                'août': 8,\r\n",
        "                'septembre': 9,\r\n",
        "                'octobre': 10,\r\n",
        "                'novembre': 11,\r\n",
        "                'décembre': 12\r\n",
        "            }            \r\n",
        "            return dictionary[month]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qWn_16ThyokX"
      },
      "source": [
        "\r\n",
        "def add_effective_date(df, body_column):\r\n",
        "    \"\"\"A function that searches the document bodies for fromDate and endDates\r\n",
        "    and extracts datetime objects into new columns\"\"\"\r\n",
        "    \r\n",
        "    def get_dates(df_row):\r\n",
        "        #start date matches ~46 out of 48 test cases\r\n",
        "        #group 6 = month\r\n",
        "        #group 7 = year\r\n",
        "        pattern = r\"((cette|la présente) (convention collective de travail|CCT)|elle).+(entre en vigueur|à partir|produit ses effets|s'étend|sort ses effets|prend cours|\\bdéterminée|allant).{1,19}(le\\b|du\\b|de\\b|au\\b).{1,5}(janvier|février|mars|avril|mai|juin|juillet|août|septembre|octobre|novembre|décembre) ([0-9]{4})\"\r\n",
        "        pattern = r\"(cette|la présente) (convention collective de travail|CCT|elle).+(entre en vigueur|à partir|produit ses effets|s'étend|sort ses effets|prend cours|\\bdéterminée|allant).{1,19}(le\\b|du\\b|de\\b|au\\b).{1,5}(janvier|février|mars|avril|mai|juin|juillet|août|septembre|octobre|novembre|décembre) ([0-9]{4})\"\r\n",
        "        #pattern = r\"[cette|la présente] [convention collective de travail|CCT]|[elle].+[entre en vigueur|à partir|produit ses effets|s'étend|sort ses effets|prend cours|\\bdéterminée|allant].{1,19}[le\\b|du\\b|de\\b|au\\b].{1,5}(janvier|février|mars|avril|mai|juin|juillet|août|septembre|octobre|novembre|décembre) ([0-9]{4})\"\r\n",
        "        \r\n",
        "        # finds the first instance of the regex in the body\r\n",
        "        match = re.search(pattern, df_row[body_column], flags=re.IGNORECASE)\r\n",
        "        if match:\r\n",
        "            try:\r\n",
        "                #df_row['fromDate'] = datetime.date(int(match[7]), get_month_digit(match[6]), 1)\r\n",
        "                df_row['fromDate'] = datetime.date(int(match[6]), get_month_digit(match[5]), 1)\r\n",
        "                #df_row['fromDate'] = datetime.date(int(match[1]), get_month_digit(match[0]), 1)\r\n",
        "                #testing purpose fm\r\n",
        "                print(match[0])\r\n",
        "                print(match[1])\r\n",
        "                print(match[2])\r\n",
        "                print(match[3])\r\n",
        "                print(match[4])\r\n",
        "                print(match[5])\r\n",
        "                print(match[6])\r\n",
        "                #print(f'{type(match)}')\r\n",
        "                #for i in range(10):\r\n",
        "                #  print(match[i])\r\n",
        "            except:\r\n",
        "                pass\r\n",
        "        else:\r\n",
        "            df_row['fromDate'] = np.nan\r\n",
        "        \r\n",
        "        #end date matches 47 of 48\r\n",
        "        #group 6 = month\r\n",
        "        #group 7 = year\r\n",
        "        pattern = r\"((cette|la présente) (convention collective de travail|CCT)|elle).+([0-9]{4}|cesse de produire ses effets|cesse d'être en vigueur|cesse ses effets|prend fin|expire|conclue jusq|prend.{1,25}fin).+(le\\b|au\\b).{1,5}(janvier|février|mars|avril|mai|juin|juillet|août|septembre|octobre|novembre|décembre) ([0-9]{4})\"\r\n",
        "        pattern = r\"(cette|la présente) (convention collective de travail|CCT|elle).+([0-9]{4}|cesse de produire ses effets|cesse d'être en vigueur|cesse ses effets|prend fin|expire|conclue jusq|prend.{1,25}fin).+(le\\b|au\\b).{1,5}(janvier|février|mars|avril|mai|juin|juillet|août|septembre|octobre|novembre|décembre) ([0-9]{4})\"\r\n",
        "        #pattern = r\"[cette|la présente] [convention collective de travail|CCT)|elle].+[[0-9]{4}|cesse de produire ses effets|cesse d'être en vigueur|cesse ses effets|prend fin|expire|conclue jusq|prend.{1,25}fin].+[le\\b|au\\b].{1,5}(janvier|février|mars|avril|mai|juin|juillet|août|septembre|octobre|novembre|décembre) ([0-9]{4})\"\r\n",
        "        #pattern = r\"[cette|la présente] [convention collective de travail|CCT)|elle].+[[0-9]{4}|cesse de produire ses effets|cesse d'être en vigueur|cesse ses effets|prend fin|expire|conclue jusq|prend.{1,25}fin]\"\r\n",
        "        #pattern += r\".+[le\\b|au\\b].{1,5}(janvier|février|mars|avril|mai|juin|juillet|août|septembre|octobre|novembre|décembre) ([0-9]{4})\"\r\n",
        "        \r\n",
        "        match = re.search(pattern, df_row[body_column], flags=re.IGNORECASE)\r\n",
        "        if match:\r\n",
        "            try:\r\n",
        "                #df_row['endDate'] = datetime.date(int(match[7]), get_month_digit(match[6]), 28)\r\n",
        "                df_row['endDate'] = datetime.date(int(match[6]), get_month_digit(match[5]), 28)\r\n",
        "                #df_row['endDate'] = datetime.date(int(match[1]), get_month_digit(match[0]), 28)\r\n",
        "                #testing purpose fm   \r\n",
        "                #print(f'{type(match)}') \r\n",
        "                print(match[0])\r\n",
        "                print(match[1])\r\n",
        "                print(match[2])\r\n",
        "                print(match[3])\r\n",
        "                print(match[4])\r\n",
        "                print(match[5])\r\n",
        "                print(match[6])     \r\n",
        "                #for i in range(10):\r\n",
        "                #  print(match[i])\r\n",
        "            except:\r\n",
        "                pass\r\n",
        "        else:\r\n",
        "            df_row['endDate'] = np.nan\r\n",
        "        \r\n",
        "        #there might have been an erroneous match of endDate\r\n",
        "        #which should be overwritten by None when durée idéterminée is detected\r\n",
        "        pattern = r\"((cette|la présente) (convention collective de travail|CCT)|elle).+(durée indéterminée|([0-9]{4}.+à l'exception))\"\r\n",
        "        match = re.search(pattern, df_row[body_column], flags=re.IGNORECASE)\r\n",
        "        if match:\r\n",
        "            df_row['endDate'] = np.nan\r\n",
        "        \r\n",
        "        return df_row\r\n",
        "    \r\n",
        "    df = df.apply(get_dates, axis=1)\r\n",
        "\r\n",
        "    return df"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "geTgUeUJ_Eri"
      },
      "source": [
        "#extracting reference\r\n",
        "#Pendant la durée de cette CCT tous les jours de chômage temporaire ne seront pas pris en compte dans le calcul de la période de maximum soixante jours par année de article 2 de la CCT du 17 septembre 2019 conclue au sein de la Commission Paritaire de l’industrie chimique relative à la sécurité d'existence (numéro d'enregistrement 154419).\r\n",
        "\r\n",
        "def extract_reference(lst):\r\n",
        "  output_lst = []\r\n",
        "  for item in lst: \r\n",
        "    pattern =r\".*(article )([0-9]{2}).*([0-9]{2}) (janvier|février|mars|avril|mai|juin|juillet|août|septembre|octobre|novembre|décembre) ([0-9]{4}).*(numéro d'enregistrement)([0-9]{6}).*\"\r\n",
        "    match = re.search(pattern, item, flags=re.IGNORECASE)\r\n",
        "    if match:\r\n",
        "      output_str = \"article \"+str(match[2])+\",\"+str(match[3])+\"/\"+get_month_digit(match[4])+\"/\"+str(match[5])+\", n. \"+match[7]\r\n",
        "      output_lst.append(output_str)\r\n",
        "    else:\r\n",
        "      pass\r\n",
        "  return '\\n'.join(output_lst)\r\n"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pw1AoMGaBBlz",
        "outputId": "4b241791-14de-440a-ce7a-1d2fc2f22c3b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "df['reference'] = df['texts_lst'].apply(extract_reference)\r\n",
        "df['reference']"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "34    \n",
              "35    \n",
              "36    \n",
              "37    \n",
              "38    \n",
              "Name: reference, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-eyc6d2ByrAn"
      },
      "source": [
        "def extract(df):\r\n",
        "\t'''overall regex extraction functioin'''\r\n",
        "\r\n",
        "\tdf = add_scope(df)\r\n",
        "\r\n",
        "\tdf = add_effective_date(df)\r\n",
        "\r\n",
        "\treturn df"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZFDCiaWamxNC",
        "outputId": "948f00dd-16ed-4341-cbca-d861af3e5a48"
      },
      "source": [
        "df =  add_effective_date(df, \"doc_bodies\")\r\n",
        "#df.loc[:,['txt_name', 'texts_lst', 'depotFr', 'fromDate', 'endDate']]\r\n"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "La présente convention collective de travail, remplace à partir du 1” janvier 2018\n",
            "La présente\n",
            "convention collective de travail\n",
            "à partir\n",
            "du\n",
            "janvier\n",
            "2018\n",
            "La présente convention collective de travail entre en vigueur le 1er janvier 2018\n",
            "La présente\n",
            "convention collective de travail\n",
            "entre en vigueur\n",
            "le\n",
            "janvier\n",
            "2018\n",
            "La présente convention collective de travail entre en vigueur le 1er janvier 2018 pour les allocations afférentes à l'exercice 2018 et cesse d'être en vigueur le 31 décembre 2018\n",
            "La présente\n",
            "convention collective de travail\n",
            "cesse d'être en vigueur\n",
            "le\n",
            "décembre\n",
            "2018\n",
            "La présente convention collective de travail produit ses effets le 1” juillet 2018\n",
            "La présente\n",
            "convention collective de travail\n",
            "produit ses effets\n",
            "le\n",
            "juillet\n",
            "2018\n",
            "La présente convention collective de travail entre en vigueur le 1” juillet 2018\n",
            "La présente\n",
            "convention collective de travail\n",
            "entre en vigueur\n",
            "le\n",
            "juillet\n",
            "2018\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H2PzphfD67lX"
      },
      "source": [
        "# Preprocessing for NLP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R1EBJ4N3TaKL"
      },
      "source": [
        "from pandas import Series\r\n",
        "\r\n",
        "from nltk.corpus import wordnet\r\n",
        "from nltk.tokenize import word_tokenize\r\n",
        "from nltk.tag import pos_tag\r\n",
        "from nltk.stem import WordNetLemmatizer\r\n",
        "\r\n",
        "# import entirely spacy to create Doc objects through nlp\r\n",
        "import spacy\r\n",
        "from spacy import load, lang\r\n",
        "\r\n",
        "from wordcloud import WordCloud\r\n",
        "\r\n",
        "from collections import Counter\r\n",
        "\r\n",
        "from typing import List\r\n",
        "from typing import Dict\r\n",
        "\r\n",
        "# WARNINGS\r\n",
        "# W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found\r\n",
        "# I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine\r\n",
        "\r\n",
        "# GLOBAL VARIABLES\r\n",
        "NAMED_ENTITIES = ['microsoft']\r\n",
        "\r\n",
        "#testing\r\n",
        "from os import getcwd as cwd\r\n",
        "from os.path import dirname as dir\r\n",
        "from os.path import join\r\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nC6sZ-BPTcUs"
      },
      "source": [
        "def lemmatize(text_tokens: List[str]) -> List[str]:\r\n",
        "    def get_wordnet_pos(word):\r\n",
        "        \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\r\n",
        "        tag = pos_tag([word])[0][1][0].upper()\r\n",
        "        tag_dict = {\"J\": wordnet.ADJ,\r\n",
        "                    \"N\": wordnet.NOUN,\r\n",
        "                    \"V\": wordnet.VERB,\r\n",
        "                    \"R\": wordnet.ADV}\r\n",
        "        return tag_dict.get(tag, wordnet.NOUN)\r\n",
        "\r\n",
        "    # Instantiate the WordNetLemmatizer\r\n",
        "    wordnet_lemmatizer = WordNetLemmatizer()\r\n",
        "    # Lemmatize all tokens into a new list: lemmatized\r\n",
        "    texts_lemmatized = [wordnet_lemmatizer.lemmatize(t, get_wordnet_pos(t)) for t in text_tokens]\r\n",
        "    return texts_lemmatized\r\n",
        "\r\n",
        "\r\n",
        "def filter_words(texts_lemmatized: List[List[str]], freq_min=None, freq_max=None):\r\n",
        "    frequency_absolute = Counter([item for elem in texts_lemmatized for item in elem])\r\n",
        "    wordcloud = WordCloud(width=1000, height=500).generate_from_frequencies(frequency_absolute)\r\n",
        "    frequency_relative = wordcloud.words_\r\n",
        "    if freq_min is not None and freq_min > 0 and freq_min < 1:\r\n",
        "        rel_freq_filtered = {k: v for k, v in frequency_relative.items() if v > freq_min}\r\n",
        "    if freq_max is not None and freq_max > 0 and freq_max < 1:\r\n",
        "        rel_freq_filtered = {k: v for k, v in frequency_relative.items() if v < freq_max}\r\n",
        "    texts_filtered = [[t for t in pub_lem if t in rel_freq_filtered.keys()] for pub_lem in texts_lemmatized]\r\n",
        "    # testing part\r\n",
        "    f_abs_updated =Counter([ item for elem in texts_filtered for item in elem])   \r\n",
        "    wordcloud_updated = WordCloud(width=1000, height=500).generate_from_frequencies(f_abs_updated)\r\n",
        "    f_rel_updated = wordcloud_updated.words_\r\n",
        "    return texts_filtered, f_abs_updated, f_rel_updated, wordcloud_updated\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ib8xc7-F5tGY"
      },
      "source": [
        "class Preprocess:\r\n",
        "    def __init__(self, nlp_model='en_core_web_md'):\r\n",
        "        self.nlp = load(nlp_model)\r\n",
        "        if 'en_' in nlp_model:\r\n",
        "          self.stop_words = lang.en.stop_words.STOP_WORDS\r\n",
        "        elif 'fr_' in nlp_model:\r\n",
        "          self.stop_words = lang.fr.stop_words.STOP_WORDS\r\n",
        "        self.named_entities = set(NAMED_ENTITIES)\r\n",
        "\r\n",
        "    def get_named_entities(self, texts: Series, inplace=True) -> set:\r\n",
        "        # creating a single ner set\r\n",
        "        nes = set()\r\n",
        "        # function to extract NER from text\r\n",
        "        def get_named_entities(text) -> set:\r\n",
        "            doc = self.nlp(text)\r\n",
        "            named_entities = set([ent.text for ent in doc.ents])\r\n",
        "            return named_entities\r\n",
        "        [[nes.add(n) for n in get_named_entities(text)] for text in texts]\r\n",
        "        # adding predefined NER\r\n",
        "        [nes.add(n) for n in self.named_entities]\r\n",
        "        if inplace:\r\n",
        "            self.named_entities = nes\r\n",
        "        return nes\r\n",
        "\r\n",
        "    def tokenize_text(self, text:str, stop_words: List[str] = None, named_entities: List[str] = None,\r\n",
        "                   lenght_min: int=2) -> List[str]:\r\n",
        "        if stop_words is None:\r\n",
        "            stop_words = self.stop_words\r\n",
        "        if named_entities is None:\r\n",
        "            named_entities = self.named_entities\r\n",
        "        text = text.replace(\"\\n\", \" \")\r\n",
        "        # split string into words (tokens)\r\n",
        "        tokens = word_tokenize(text.lower())\r\n",
        "        # keep strings with only alphabets\r\n",
        "        tokens = [t for t in tokens if t.isalpha()]\r\n",
        "        tokens = lemmatize(tokens)\r\n",
        "        # remove short words, they're probably not useful\r\n",
        "        tokens = [t for t in tokens if len(t) > lenght_min]\r\n",
        "        # remove stopwords\r\n",
        "        tokens = [t for t in tokens if t not in stop_words]\r\n",
        "        # remove\r\n",
        "        tokens = [t for t in tokens if t not in named_entities]\r\n",
        "        return tokens\r\n",
        "\r\n",
        "    def clean_text(self, text:str, stop_words: List[str] = None, named_entities: List[str] = None,\r\n",
        "                   lenght_min: int=2) -> str:\r\n",
        "        tokens = self.tokenize_text(text, stop_words, named_entities, lenght_min)\r\n",
        "        text_cleaned = \" \".join(tokens)\r\n",
        "        return text_cleaned\r\n",
        "\r\n",
        "    def tokenize_texts(self, texts:Series, stop_words: List[str] = None, named_entities: List[str] = None,\r\n",
        "                   lenght_min: int=2) -> List[List[str]]:\r\n",
        "        texts_tokens = []\r\n",
        "        for text in texts:\r\n",
        "            texts_tokens += [self.tokenize_text(text, stop_words, named_entities, lenght_min)]\r\n",
        "        return texts_tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UzM0CO3T7Jp5"
      },
      "source": [
        "! python -m spacy download fr_core_news_md"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-gSiOJ547ivj"
      },
      "source": [
        "import spacy\r\n",
        "nlp = spacy.load('fr_core_news_md')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m7pS6KZYHixh"
      },
      "source": [
        "preprocess = Preprocess(nlp_model='fr_core_news_md')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dZ1u8dKqIOi2"
      },
      "source": [
        "# example of FR stopwords from the set\r\n",
        "list(preprocess.stop_words)[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ZKK_K-6O8Dh"
      },
      "source": [
        "# importing nltk and downloading additional required packages\r\n",
        "import nltk\r\n",
        "nltk.download('punkt')\r\n",
        "nltk.download('averaged_perceptron_tagger')\r\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nlVBC4-mOu4K"
      },
      "source": [
        "#cleaning text before extracting word clouds\r\n",
        "responses2[\"bodies_cleaned\"] = responses2[\"doc_bodies\"].apply(preprocess.clean_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gfpoq2YLL4na"
      },
      "source": [
        "# Word cloud labelling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qsmt70TiL7FV"
      },
      "source": [
        "#extracting all words with relative frequency > 0.5\r\n",
        "all_ts = preprocess.tokenize_texts(responses2.bodies_cleaned)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQ4gk4WKTHMO"
      },
      "source": [
        "texts, f_abs, f_rel, wordcloud = filter_words(all_ts, freq_min=0.5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0tNIVnCaTmVs"
      },
      "source": [
        "wordcloud.to_image()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EilnPDhRTs9z"
      },
      "source": [
        "# adding most frequent words as stop words\r\n",
        "[preprocess.stop_words.add(w) for w in list(f_rel.keys())]\r\n",
        "[w for w in list(preprocess.stop_words) if w in list(f_rel.keys())]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hiB25mXkUjIG"
      },
      "source": [
        "[w for w in list(preprocess.stop_words) if w in list(f_rel.keys())]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XtLYJqkRXJOm"
      },
      "source": [
        "themes_keywords = pd.DataFrame(columns=['themeFr', 'frequency_absolute', 'frequency_relative'])\r\n",
        "for t in themes_fr:\r\n",
        "  mask = responses2.dropna(axis=0, subset=['themesFr']).themesFr.map(lambda x: t in x)\r\n",
        "  r_t = responses2.dropna(axis=0, subset=['themesFr'])[mask]\r\n",
        "  t_ts = preprocess.tokenize_texts(r_t[\"bodies_cleaned\"])\r\n",
        "  t_texts, t_f_abs, t_f_rel, t_wordcloud = filter_words(t_ts, freq_min=0.05, freq_max=0.95)\r\n",
        "  themes_keywords = themes_keywords.append({'themeFr':t, 'frequency_absolute':t_f_abs, 'frequency_relative':t_f_rel}, ignore_index=True)\r\n",
        "  print(f'Kewyords for {t} added, {len(r_t)} documents found')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dLtvqXPWgKgH"
      },
      "source": [
        "for t in themes_fr:\r\n",
        "  mask = responses2.dropna(axis=0, subset=['themesFr']).themesFr.map(lambda x: t in x)\r\n",
        "  r_t = responses2.dropna(axis=0, subset=['themesFr'])[mask]\r\n",
        "  print(f'{len(r_t)} contained for {t}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "33e2Jk09f74p"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pqhn8BJPYocl"
      },
      "source": [
        "themes_keywords.to_csv('/content/drive/My Drive/Becode/Kpmg/labeling.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJpQLbYqZk74"
      },
      "source": [
        "themes_keywords.loc[:, ['themeFr','frequency_relative']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SpotjOgBmLpS"
      },
      "source": [
        "# Classification model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xGZ0nUEXmd2j"
      },
      "source": [
        "from pickle import load\r\n",
        "\r\n",
        "with open(os.path.join(KPMG_PATH, \"clas_new.pkl\"), 'rb') as f:\r\n",
        "  df = load(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n62CpQau9j-Y"
      },
      "source": [
        "df.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8zKReQwv9xc9"
      },
      "source": [
        "df[\"body_cleaned\"] = df[\"docBodyFr\"].apply(preprocess.clean_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SF6EgPpW-cGJ"
      },
      "source": [
        "#generating all possible FR themes (replacing previous list of 53 items)\r\n",
        "themes_fr = []\r\n",
        "for r in  df.themesFr:\r\n",
        "    if r is not None:\r\n",
        "        for t in r:\r\n",
        "            if t not in themes_fr:\r\n",
        "                themes_fr += [t]\r\n",
        "len(themes_fr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-IrpIu9a_IbD"
      },
      "source": [
        "# showing only 27 unique themes against 54 available\r\n",
        "df.dropna(axis=0, subset=['themesFr'], inplace=True)\r\n",
        "df['theme'] = df['themesFr'].apply(lambda x: x[0] if len(x) == 1 else None)\r\n",
        "len(df.theme.unique())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WMO4woG9JDnc"
      },
      "source": [
        "for t in themes_fr:\r\n",
        "\tdf.dropna(axis=0, subset=['themesFr'], inplace=True)\r\n",
        "\tdf[t] = df['themesFr'].apply(lambda x: 1 if t in x else 0)\r\n",
        "df.columns"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}