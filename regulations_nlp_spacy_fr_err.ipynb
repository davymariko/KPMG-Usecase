{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "regulations_nlp_spacy_fr_err.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyN8C7yaL9pH/Q0MwOnzi8Yr"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kuKWsJV27Xe2"
      },
      "source": [
        "# Importing and preprocessing of data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OHoCSdHswimk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d1244c9-96c3-4d2d-8197-af0b75171b13"
      },
      "source": [
        "from google.colab import drive\r\n",
        "\r\n",
        "drive.mount('/content/drive/')\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMVNjvlExvOf"
      },
      "source": [
        "TXTS_PATH = '/content/drive/My Drive/Becode/Kpmg/selection_fr'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AxwxF21YxnUj"
      },
      "source": [
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JZx91aOoyi1d"
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J_t3pBX-xF7U"
      },
      "source": [
        "RESPONSES_PATH = '/content/drive/My Drive/Becode/Kpmg/responses_2018_now.json'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DkiARh2Oxg4o"
      },
      "source": [
        "txt_filenames = [f for f in os.listdir(TXTS_PATH) if f.endswith(\".txt\")]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZddQl2FVydlE"
      },
      "source": [
        "#reading json file to retrieve tags\r\n",
        "responses = pd.read_json(RESPONSES_PATH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "ycPxUepwyxtL",
        "outputId": "5af3ac11-4a11-4c1a-af7f-73e1a2665159"
      },
      "source": [
        "#filtering only relevant columns\r\n",
        "columns = ['jcId', 'jcFr', 'titleFr', 'themesFr', 'scopeFr', 'noScopeFr', 'documentLink']\r\n",
        "responses2 = responses.copy(deep=True).loc[:,columns]\r\n",
        "#getting txt_name from original pdf name\r\n",
        "responses2[\"txt_name\"] = responses2[\"documentLink\"]\r\n",
        "responses2[\"txt_name\"] = responses2[\"txt_name\"].str.replace(\"/\",\"-\")\r\n",
        "responses2[\"txt_name\"] = responses2[\"txt_name\"].str.replace(\".pdf\",\"_FR.txt\")\r\n",
        "#filtering json only for selected files and columns\r\n",
        "columns += ['txt_name']\r\n",
        "responses2 = responses2.loc[responses2.txt_name.isin(txt_filenames), columns]\r\n",
        "responses2.head(2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>jcId</th>\n",
              "      <th>jcFr</th>\n",
              "      <th>titleFr</th>\n",
              "      <th>themesFr</th>\n",
              "      <th>scopeFr</th>\n",
              "      <th>noScopeFr</th>\n",
              "      <th>documentLink</th>\n",
              "      <th>txt_name</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>1110000</td>\n",
              "      <td>COMMISSION PARITAIRE DES CONSTRUCTIONS METALLI...</td>\n",
              "      <td>modification du régime de pension sectoriel so...</td>\n",
              "      <td>[PENSIONS COMPÉMENTAIRES ET ASSURANCES GROUPES]</td>\n",
              "      <td>None</td>\n",
              "      <td>[les employeurs et ouvriers des entreprises ex...</td>\n",
              "      <td>111/111-2018-013525.pdf</td>\n",
              "      <td>111-111-2018-013525_FR.txt</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>1110000</td>\n",
              "      <td>COMMISSION PARITAIRE DES CONSTRUCTIONS METALLI...</td>\n",
              "      <td>allocation spéciale compensatoire</td>\n",
              "      <td>[PRIME SYNDICALE]</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>111/111-2018-012196.pdf</td>\n",
              "      <td>111-111-2018-012196_FR.txt</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       jcId  ...                    txt_name\n",
              "34  1110000  ...  111-111-2018-013525_FR.txt\n",
              "35  1110000  ...  111-111-2018-012196_FR.txt\n",
              "\n",
              "[2 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XlXwURIgy2uP",
        "outputId": "29dffbfb-1f54-4994-aedb-7d8cfa18c645"
      },
      "source": [
        "#generating all possible FR themes\r\n",
        "themes_fr = []\r\n",
        "for r in  responses2.themesFr:\r\n",
        "    if r is not None:\r\n",
        "        for t in r:\r\n",
        "            if t not in themes_fr:\r\n",
        "                themes_fr += [t]\r\n",
        "len(themes_fr)\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "53"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r2kAiO8Uy9Eu"
      },
      "source": [
        "#code to retrieve the entire body\r\n",
        "#%%\r\n",
        "for filename in responses2.txt_name.to_list():\r\n",
        "    # filename = responses2.txt_name.to_list()[10] #test\r\n",
        "    file_path = os.path.join(TXTS_PATH, filename)\r\n",
        "    with open(file_path, 'r', encoding=\"utf8\") as f:\r\n",
        "        #to retrieve entire body not necessary\r\n",
        "        responses2.loc[responses2.txt_name == filename, \"doc_bodies\"] = f.read()\r\n",
        "        f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iox0MB-33HOD"
      },
      "source": [
        "t = themes_fr[0]\r\n",
        "mask = responses2.dropna(axis=0, subset=['themesFr']).themesFr.map(lambda x: t in x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NUAfSmIf16p-",
        "outputId": "cb70bc62-7fcd-4195-d170-20d790c42428"
      },
      "source": [
        "len(responses2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "549"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0kHR6RAx2OK0",
        "outputId": "2ab8313b-8710-4d9d-b9e0-313bc1b74582"
      },
      "source": [
        " len(responses2.dropna(axis=0, subset=['themesFr'])[map])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "35"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H2PzphfD67lX"
      },
      "source": [
        "# Preprocessing for NLP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ib8xc7-F5tGY"
      },
      "source": [
        "from pandas import Series\r\n",
        "\r\n",
        "from nltk.corpus import wordnet\r\n",
        "from nltk.tokenize import word_tokenize\r\n",
        "from nltk.tag import pos_tag\r\n",
        "from nltk.stem import WordNetLemmatizer\r\n",
        "\r\n",
        "# import entirely spacy to create Doc objects through nlp\r\n",
        "import spacy\r\n",
        "from spacy import load, lang\r\n",
        "\r\n",
        "from wordcloud import WordCloud\r\n",
        "\r\n",
        "from collections import Counter\r\n",
        "\r\n",
        "from typing import List\r\n",
        "from typing import Dict\r\n",
        "\r\n",
        "# WARNINGS\r\n",
        "# W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found\r\n",
        "# I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine\r\n",
        "\r\n",
        "# GLOBAL VARIABLES\r\n",
        "NAMED_ENTITIES = ['microsoft']\r\n",
        "\r\n",
        "#testing\r\n",
        "from os import getcwd as cwd\r\n",
        "from os.path import dirname as dir\r\n",
        "from os.path import join\r\n",
        "import pandas as pd\r\n",
        "\r\n",
        "\r\n",
        "def lemmatize(text_tokens: List[str]) -> List[str]:\r\n",
        "    def get_wordnet_pos(word):\r\n",
        "        \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\r\n",
        "        tag = pos_tag([word])[0][1][0].upper()\r\n",
        "        tag_dict = {\"J\": wordnet.ADJ,\r\n",
        "                    \"N\": wordnet.NOUN,\r\n",
        "                    \"V\": wordnet.VERB,\r\n",
        "                    \"R\": wordnet.ADV}\r\n",
        "        return tag_dict.get(tag, wordnet.NOUN)\r\n",
        "\r\n",
        "    # Instantiate the WordNetLemmatizer\r\n",
        "    wordnet_lemmatizer = WordNetLemmatizer()\r\n",
        "    # Lemmatize all tokens into a new list: lemmatized\r\n",
        "    texts_lemmatized = [wordnet_lemmatizer.lemmatize(t, get_wordnet_pos(t)) for t in text_tokens]\r\n",
        "    return texts_lemmatized\r\n",
        "\r\n",
        "\r\n",
        "#DEV NOTE: not used\r\n",
        "def filter_words(texts_lemmatized: List[List[str]], freq_min=None, freq_max=None) -> List[List[str]]:\r\n",
        "    frequency_absolute = Counter([item for elem in texts_lemmatized for item in elem])\r\n",
        "    wordcloud = WordCloud(width=1000, height=500).generate_from_frequencies(frequency_absolute)\r\n",
        "    frequency_relative = wordcloud.words_\r\n",
        "    if freq_min is not None and freq_min > 0 and freq_min < 1:\r\n",
        "        rel_freq_filtered = {k: v for k, v in frequency_relative.items() if v > freq_min}\r\n",
        "    if freq_max is not None and freq_max > 0 and freq_max < 1:\r\n",
        "        rel_freq_filtered = {k: v for k, v in frequency_relative.items() if v < freq_max}\r\n",
        "    texts_filtered = [[t for t in pub_lem if t in rel_freq_filtered.keys()] for pub_lem in texts_lemmatized]\r\n",
        "    return texts_filtered\r\n",
        "\r\n",
        "\r\n",
        "class Preprocess:\r\n",
        "    def __init__(self, nlp_model='en_core_web_md'):\r\n",
        "        self.nlp = load(nlp_model)\r\n",
        "        self.stop_words = lang.en.stop_words.STOP_WORDS\r\n",
        "        self.named_entities = set(NAMED_ENTITIES)\r\n",
        "\r\n",
        "    def get_named_entities(self, texts: Series, inplace=True) -> set:\r\n",
        "        # creating a single ner set\r\n",
        "        nes = set()\r\n",
        "        # function to extract NER from text\r\n",
        "        def get_named_entities(text) -> set:\r\n",
        "            doc = self.nlp(text)\r\n",
        "            named_entities = set([ent.text for ent in doc.ents])\r\n",
        "            return named_entities\r\n",
        "        [[nes.add(n) for n in get_named_entities(text)] for text in texts]\r\n",
        "        # adding predefined NER\r\n",
        "        [nes.add(n) for n in self.named_entities]\r\n",
        "        if inplace:\r\n",
        "            self.named_entities = nes\r\n",
        "        return nes\r\n",
        "\r\n",
        "    def tokenize_text(self, text:str, stop_words: List[str] = None, named_entities: List[str] = None,\r\n",
        "                   lenght_min: int=2) -> List[str]:\r\n",
        "        if stop_words is None:\r\n",
        "            stop_words = self.stop_words\r\n",
        "        if named_entities is None:\r\n",
        "            named_entities = self.named_entities\r\n",
        "        text = text.replace(\"\\n\", \" \")\r\n",
        "        # split string into words (tokens)\r\n",
        "        tokens = word_tokenize(text.lower())\r\n",
        "        # keep strings with only alphabets\r\n",
        "        tokens = [t for t in tokens if t.isalpha()]\r\n",
        "        tokens = lemmatize(tokens)\r\n",
        "        # remove short words, they're probably not useful\r\n",
        "        tokens = [t for t in tokens if len(t) > lenght_min]\r\n",
        "        # remove stopwords\r\n",
        "        tokens = [t for t in tokens if t not in stop_words]\r\n",
        "        # remove\r\n",
        "        tokens = [t for t in tokens if t not in named_entities]\r\n",
        "        return tokens\r\n",
        "\r\n",
        "    def clean_text(self, text:str, stop_words: List[str] = None, named_entities: List[str] = None,\r\n",
        "                   lenght_min: int=2) -> str:\r\n",
        "        tokens = self.tokenize_text(text, stop_words, named_entities, lenght_min)\r\n",
        "        text_cleaned = \" \".join(tokens)\r\n",
        "        return text_cleaned\r\n",
        "\r\n",
        "    def tokenize_texts(self, texts:Series, stop_words: List[str] = None, named_entities: List[str] = None,\r\n",
        "                   lenght_min: int=2) -> List[List[str]]:\r\n",
        "        texts_tokens = []\r\n",
        "        for text in texts:\r\n",
        "            texts_tokens = texts_tokens.append(self.tokenize_text(text, stop_words, named_entities, lenght_min))\r\n",
        "        return texts_tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UzM0CO3T7Jp5",
        "outputId": "78cd86db-288a-465e-9cee-e6c0d351e0b7"
      },
      "source": [
        "! python -m spacy download fr_core_news_md"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting fr_core_news_md==2.2.5\n",
            "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/fr_core_news_md-2.2.5/fr_core_news_md-2.2.5.tar.gz (88.6MB)\n",
            "\u001b[K     |████████████████████████████████| 88.6MB 1.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from fr_core_news_md==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->fr_core_news_md==2.2.5) (2.0.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->fr_core_news_md==2.2.5) (0.8.2)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->fr_core_news_md==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->fr_core_news_md==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->fr_core_news_md==2.2.5) (3.0.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->fr_core_news_md==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->fr_core_news_md==2.2.5) (1.19.5)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->fr_core_news_md==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->fr_core_news_md==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->fr_core_news_md==2.2.5) (54.0.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->fr_core_news_md==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->fr_core_news_md==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->fr_core_news_md==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->fr_core_news_md==2.2.5) (3.7.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->fr_core_news_md==2.2.5) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->fr_core_news_md==2.2.5) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->fr_core_news_md==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->fr_core_news_md==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->fr_core_news_md==2.2.5) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->fr_core_news_md==2.2.5) (3.4.0)\n",
            "Building wheels for collected packages: fr-core-news-md\n",
            "  Building wheel for fr-core-news-md (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fr-core-news-md: filename=fr_core_news_md-2.2.5-cp37-none-any.whl size=90338490 sha256=c22cf55a32c8efc54aec17c961c4b7e8f8a6162707d30ff12f5fe89aac7e5790\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-872xxiyd/wheels/c6/18/b6/f628642acc7872a53cf81269dd1c394d96da69564ccfac5425\n",
            "Successfully built fr-core-news-md\n",
            "Installing collected packages: fr-core-news-md\n",
            "Successfully installed fr-core-news-md-2.2.5\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('fr_core_news_md')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 334
        },
        "id": "-gSiOJ547ivj",
        "outputId": "3865540a-e8d4-43a6-f0b2-e526ae63730c"
      },
      "source": [
        "import spacy\r\n",
        "nlp = spacy.load('fr_core_news_md')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-85-562755d3b140>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'fr_core_news_md'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/spacy/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(name, **overrides)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdepr_path\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mdeprecation_warning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW001\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdepr_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/spacy/util.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(name, **overrides)\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"exists\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Path or Path-like to model data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mload_model_from_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE050\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: [E050] Can't find model 'fr_core_news_md'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1qD-aTcn8atq",
        "outputId": "ac2ea32c-4d8e-4503-a889-a6aec512cf24"
      },
      "source": [
        "! python --version"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Python 3.7.10\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}